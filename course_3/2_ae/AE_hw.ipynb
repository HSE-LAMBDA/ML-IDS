{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will practice working with autoencoders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installations and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -q pytorch_lightning\n",
    "! pip install -q torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split, DataLoader\n",
    "from torchvision.datasets import MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple AutoEncoder implementation\n",
    "\n",
    "Firstly, there is an example of the AE implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "\n",
    "class SimpleEncoder(nn.Module):\n",
    "    def __init__(self, input_shape, code_size):\n",
    "        super().__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.code_size = code_size\n",
    "\n",
    "        # Calculate the flattened size\n",
    "        self.flattened_size = 1\n",
    "        for x in self.input_shape:\n",
    "            self.flattened_size *= x\n",
    "\n",
    "        self.input_to_representation = nn.Linear(self.flattened_size, self.code_size)\n",
    "\n",
    "    def forward(self, image_batch):\n",
    "        flattened = image_batch.view(-1, self.flattened_size)\n",
    "        representation = F.relu(self.input_to_representation(flattened))\n",
    "        return representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder\n",
    "\n",
    "class SimpleDecoder(nn.Module):\n",
    "    def __init__(self, input_shape, code_size):\n",
    "        super().__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.code_size = code_size\n",
    "\n",
    "        # Calculate the flattened size\n",
    "        self.flattened_size = 1\n",
    "        for x in self.input_shape:\n",
    "            self.flattened_size *= x\n",
    "\n",
    "        self.representation_to_output = nn.Linear(self.code_size, self.flattened_size)\n",
    "\n",
    "    def forward(self, representation):\n",
    "        flat_reconstructed = F.sigmoid(self.representation_to_output(representation))\n",
    "        reconstructed = flat_reconstructed.view(-1, *self.input_shape)\n",
    "        return reconstructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AutoEncoder\n",
    "\n",
    "class SimpleAutoEncoder(pl.LightningModule):\n",
    "    def __init__(self, input_shape, code_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters()  # save input_shape, code_size\n",
    "\n",
    "        self.input_shape = input_shape\n",
    "        self.code_size = code_size\n",
    "\n",
    "        # Calculate the flattened size\n",
    "        flattened_size = 1\n",
    "        for x in self.input_shape:\n",
    "            flattened_size *= x\n",
    "\n",
    "        self.encoder = SimpleEncoder(input_shape, code_size)\n",
    "        self.decoder = SimpleDecoder(input_shape, code_size)\n",
    "\n",
    "    def forward(self, image_batch):\n",
    "        return self.decoder(self.encoder(image_batch))\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        batch_images = batch[0]\n",
    "        reconstructed_images = self.forward(batch_images)\n",
    "        loss = F.mse_loss(reconstructed_images, batch_images)\n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        batch_images = batch[0]\n",
    "        reconstructed_images = self.forward(batch_images)\n",
    "        loss = F.mse_loss(reconstructed_images, batch_images)\n",
    "        self.log('val_loss', loss)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        batch_images = batch[0]\n",
    "\n",
    "        reconstructed_images = self.forward(batch_images)\n",
    "        loss = F.mse_loss(reconstructed_images, batch_images)\n",
    "        self.log('test_loss', loss)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(self.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data module\n",
    "\n",
    "class MNISTDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_dir='./', batch_size=64, num_workers=4):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "        # We hardcode dataset specific stuff here.\n",
    "        self.num_classes = 10\n",
    "        self.dims = (1, 28, 28)\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # download\n",
    "        MNIST(self.data_dir, train=True, download=True)\n",
    "        MNIST(self.data_dir, train=False, download=True)\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "\n",
    "        # Assign train/val datasets for use in dataloaders\n",
    "        if stage == 'fit' or stage is None:\n",
    "            mnist_full = MNIST(self.data_dir, train=True, transform=self.transform)\n",
    "            self.mnist_train, self.mnist_val = random_split(mnist_full, [55000, 5000])\n",
    "\n",
    "        # Assign test dataset for use in dataloader(s)\n",
    "        if stage == 'test' or stage is None:\n",
    "            self.mnist_test = MNIST(self.data_dir, train=False, transform=self.transform)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.mnist_train, batch_size=self.batch_size, num_workers=self.num_workers, pin_memory=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.mnist_val, batch_size=self.batch_size, num_workers=self.num_workers)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.mnist_test, batch_size=self.batch_size, num_workers=self.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "\n",
    "trainer_kwargs = {\n",
    "    'gpus': 1,\n",
    "    'max_epochs': 10,\n",
    "    'precision': 16,\n",
    "    'progress_bar_refresh_rate': 5,\n",
    "    'weights_summary': \"full\"\n",
    "}\n",
    "\n",
    "mnist_dm = MNISTDataModule()\n",
    "\n",
    "ae_kwargs = {\n",
    "    'input_shape': mnist_dm.size(),\n",
    "    'code_size': 128\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "\n",
    "model = SimpleAutoEncoder(**ae_kwargs)\n",
    "trainer = pl.Trainer(**trainer_kwargs)\n",
    "trainer.fit(model, mnist_dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "\n",
    "model.eval()\n",
    "test_loss = trainer.test(model, mnist_dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plots\n",
    "trans = transforms.ToPILImage()\n",
    "\n",
    "\n",
    "def model_plots(model):\n",
    "    model.eval()\n",
    "    for batch in mnist_dm.val_dataloader():\n",
    "        original_imgs = batch[0]\n",
    "        outputs = model(original_imgs)\n",
    "        for i in range(len(outputs)):\n",
    "            f = plt.figure()\n",
    "            f.add_subplot(1, 2, 1)\n",
    "            plt.imshow(trans(original_imgs[i]).convert(\"RGB\"))\n",
    "            plt.title('origin')\n",
    "            plt.axis('off')\n",
    "            f.add_subplot(1, 2, 2)\n",
    "            plt.imshow(trans(outputs[i]).convert(\"RGB\"))\n",
    "            plt.title('AE')\n",
    "            plt.axis('off')\n",
    "            plt.show(block=True)\n",
    "            if i == 3:\n",
    "                break\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_plots(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks that even the simplest autoencoder works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1 (1/5 points)\n",
    "\n",
    "Now it's your turn! The first task is to build a graph of the dependence of the loss on the dimension of the code (leave the rest of the parameters as in the example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_size_list = [32, 64, 128, 256, 512, 1024]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_loss_list = <<your code here>>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(code_size_list, test_loss_list)\n",
    "plt.xlabel('code size')\n",
    "plt.xlabel('loss')\n",
    "plt.title('Dependence loss on code size')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that with an increase in the size of the code, the loss falls, but if the size is too large, the loss begins to grow for the selected architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 (3/5 points)\n",
    "\n",
    "Create your own encoder and decoder models, use convolutional layers. Do not change the training parameters and try to get the smallest loss!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "\n",
    "class CoolEncoder(nn.Module):\n",
    "    <<your code here>>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Decoder\n",
    "\n",
    "class CoolDecoder(nn.Module):\n",
    "    <<your code here>>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = CoolEncoder(input_shape=mnist_dm.size(), code_size=128)\n",
    "assert encoder(torch.rand([1, 1, 28, 28])).size() == torch.Size([1, 128])\n",
    "encoder = CoolEncoder(input_shape=mnist_dm.size(), code_size=10)\n",
    "assert encoder(torch.rand([1, 1, 28, 28])).size() == torch.Size([1, 10])\n",
    "\n",
    "decoder = CoolDecoder(input_shape=mnist_dm.size(), code_size=128)\n",
    "assert decoder(torch.rand((1, 128))).size() == torch.Size([1, 1, 28, 28])\n",
    "decoder = CoolDecoder(input_shape=mnist_dm.size(), code_size=10)\n",
    "assert decoder(torch.rand((1, 10))).size() == torch.Size([1, 1, 28, 28])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AutoEncoder\n",
    "\n",
    "class CoolAutoEncoder(SimpleAutoEncoder):\n",
    "    def __init__(self, input_shape, code_size):\n",
    "        super().__init__(input_shape, code_size)\n",
    "        self.encoder = CoolEncoder(input_shape, code_size)\n",
    "        self.decoder = CoolDecoder(input_shape, code_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "\n",
    "model = CoolAutoEncoder(**ae_kwargs)\n",
    "trainer = pl.Trainer(**trainer_kwargs)\n",
    "trainer.fit(model, mnist_dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "\n",
    "model.eval()\n",
    "test_loss = trainer.test(model, mnist_dm)[0]['test_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignment\n",
    "\n",
    "if test_loss <= 0.002:\n",
    "    print('BEYOND GODLIKE!!! (3 point)')\n",
    "elif test_loss <= 0.0035:\n",
    "    print('GODLIKE! (2.5 point)')\n",
    "elif test_loss <= 0.005:\n",
    "    print('Unstoppable (2 point)')\n",
    "elif test_loss <= 0.065:\n",
    "    print('Dominating (1 point)')\n",
    "elif test_loss > 0.08:\n",
    "    print('Try again =)')\n",
    "    assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots\n",
    "\n",
    "model_plots(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Denoising\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STD = 0.005\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise_generator(size, std):\n",
    "    return torch.normal(0, std, size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3 (1/5 points)\n",
    "\n",
    "fill gaps to obtain denoising model (you could previous models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenoisingAutoEncoder(SimpleAutoEncoder):\n",
    "    def __init__(self, input_shape, code_size):\n",
    "        super().__init__(input_shape, code_size)\n",
    "        self.encoder = <<your code here>>\n",
    "        self.decoder = <<your code here>>\n",
    "\n",
    "    def forward(self, image_batch):\n",
    "        if self.training:\n",
    "            noise = noise_generator(image_batch.size(), STD)\n",
    "            if image_batch.is_cuda:\n",
    "                noise = noise.cuda()\n",
    "            image_batch = image_batch + noise\n",
    "        x = self.encoder(image_batch)\n",
    "        return self.decoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DenoisingAutoEncoder(**ae_kwargs)\n",
    "trainer = pl.Trainer(**trainer_kwargs)\n",
    "trainer.fit(model, mnist_dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "test_loss = trainer.test(model, mnist_dm)[0]['test_loss']\n",
    "assert test_loss <= 0.003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denoising_model_plots(model):\n",
    "    model.eval()\n",
    "    for batch in mnist_dm.val_dataloader():\n",
    "        original_imgs = batch[0]\n",
    "        noised_imgs = batch[0] + noise_generator(original_imgs.size(), STD)\n",
    "        outputs = model(noised_imgs)\n",
    "        for i in range(len(outputs)):\n",
    "            f = plt.figure()\n",
    "            f.add_subplot(1, 3, 1)\n",
    "            plt.imshow(trans(noised_imgs[i]).convert(\"RGB\"))\n",
    "            plt.title('noised origin')\n",
    "            f.add_subplot(1, 3, 2)\n",
    "            plt.imshow(trans(outputs[i]).convert(\"RGB\"))\n",
    "            plt.title('AE denoising')\n",
    "            f.add_subplot(1, 3, 3)\n",
    "            plt.imshow(trans(original_imgs[i]).convert(\"RGB\"))\n",
    "            plt.title('origin')\n",
    "            if i == 3:\n",
    "                break\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "denoising_model_plots(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some links\n",
    "\n",
    "* https://www.youtube.com/watch?v=E2d8NRYt2e4\n",
    "* https://pytorch-lightning.readthedocs.io/en/stable/notebooks/course_UvA-DL/08-deep-autoencoders.html?highlight=autoencoder"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
