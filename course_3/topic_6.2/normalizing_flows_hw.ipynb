{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e10d1dc2",
   "metadata": {},
   "source": [
    "In this notebook we will practice working with Normalizing Flows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182ce071",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Installations and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd0736f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "! pip install -q pytorch_lightning\n",
    "! pip install -q torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291290ab",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import distributions\n",
    "from torch.distributions.transformed_distribution import TransformedDistribution\n",
    "from torch.distributions.uniform import Uniform\n",
    "from torch.distributions.transforms import SigmoidTransform\n",
    "from torch.distributions.transforms import AffineTransform\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import numpy as np\n",
    "from pylab import rcParams\n",
    "from sklearn import datasets\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "rcParams['figure.figsize'] = 10, 8\n",
    "rcParams['figure.dpi'] = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77937979",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e6c061",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# NICE implementation for MNIST\n",
    "\n",
    "As you remember, we discussed Coupling Flows and, in particular, the NICE (Non-linear Independent Component Estimation) model. The transformation in the formulas looked as follows:\n",
    "\n",
    "$$y^A = m(x^B) + x^A$$\n",
    "\n",
    "$$y^B = x^B$$\n",
    "\n",
    "Let's put such a model on our precious MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9098e5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Data loaders\n",
    "dataset = MNIST('', train=True, download=True, transform=transforms.ToTensor())\n",
    "mnist_train, mnist_val = random_split(dataset, [55000, 5000])\n",
    "train_loader = DataLoader(mnist_train, batch_size=32, num_workers=4)\n",
    "val_loader = DataLoader(mnist_val, batch_size=32, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7265850f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class StandardLogisticDistribution:\n",
    "\n",
    "    def __init__(self, data_dim=28 * 28, device='cpu'):\n",
    "        self.m = TransformedDistribution(\n",
    "            Uniform(torch.zeros(data_dim, device=device),\n",
    "                    torch.ones(data_dim, device=device)),\n",
    "            [SigmoidTransform().inv, AffineTransform(torch.zeros(data_dim, device=device),\n",
    "                                                     torch.ones(data_dim, device=device))]\n",
    "        )\n",
    "\n",
    "    def log_pdf(self, z):\n",
    "        return self.m.log_prob(z).sum(dim=1)\n",
    "\n",
    "    def sample(self):\n",
    "        return self.m.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0c2027",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class NICE(pl.LightningModule):\n",
    "    def __init__(self, distribution, data_dim=28 * 28, hidden_dim=1000, n_transformations=4):\n",
    "        super().__init__()\n",
    "        \n",
    "\n",
    "        self.distribution = distribution\n",
    "        self.n_transformations = n_transformations\n",
    "\n",
    "        # NN-transformations\n",
    "        self.m = torch.nn.ModuleList([nn.Sequential(\n",
    "            nn.Linear(data_dim // 2, hidden_dim), nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim), nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim), nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim), nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, data_dim // 2)\n",
    "        ) for i in range(n_transformations)])\n",
    "\n",
    "        self.s = torch.nn.Parameter(torch.randn(data_dim))\n",
    "        \n",
    "        # we will alternate the indices to transform,\n",
    "        # that is, one half of the input will be transformed in an even transformation\n",
    "        # and the other half in an odd transformation\n",
    "        self.idxs_even = np.full(data_dim, False)\n",
    "        self.idxs_even[::2] = True\n",
    "        self.idxs_odd = np.full(data_dim, False)\n",
    "        self.idxs_odd[1::2] = True        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        _x = x.clone()\n",
    "        \n",
    "        # normalizing flow\n",
    "        for i, m in enumerate(self.m):\n",
    "            \n",
    "            # split data\n",
    "            idxs_a = self.idxs_even if (i % 2 == 0) else self.idxs_odd\n",
    "            idxs_b = self.idxs_odd if (i % 2 == 0) else self.idxs_even\n",
    "            _x_a = _x[:, idxs_a]\n",
    "            _x_b = _x[:, idxs_b]\n",
    "            \n",
    "            # here is our formula from the lecture\n",
    "            _y_a = _x_a + m(_x_b)\n",
    "            _y_b = _x_b\n",
    "            \n",
    "            # trainsformation output\n",
    "            _x = torch.empty(_x.shape, device=_x.device)\n",
    "            _x[:, idxs_a] = _y_a\n",
    "            _x[:, idxs_b] = _y_b\n",
    "            \n",
    "        # flow output\n",
    "        y = torch.exp(self.s) * _x\n",
    "        log_det_J = torch.sum(self.s)\n",
    "            \n",
    "        return y, log_det_J\n",
    "\n",
    "    def invert(self, y):\n",
    "        \n",
    "        _y = y.clone() / torch.exp(self.s)\n",
    "        \n",
    "        # inversion\n",
    "        for i in range(len(self.m) - 1, -1, -1):\n",
    "            \n",
    "            # split data\n",
    "            idxs_a = self.idxs_even if (i % 2) == 0 else self.idxs_odd\n",
    "            idxs_b = self.idxs_odd if (i % 2) == 0 else self.idxs_even\n",
    "\n",
    "            _y_a = _y[:, idxs_a]\n",
    "            _y_b = _y[:, idxs_b]\n",
    "            \n",
    "            # here is our formula from the lecture\n",
    "            _x_a = _y_a - self.m[i](_y_b)\n",
    "            _x_b = _y_b\n",
    "            \n",
    "            # make output\n",
    "            _y = torch.empty(y.shape, device=y.device)\n",
    "            _y[:, idxs_a] = _x_a\n",
    "            _y[:, idxs_b] = _x_b\n",
    "            \n",
    "        # inverse output\n",
    "        x = _y\n",
    "            \n",
    "        return x\n",
    "    \n",
    "    def compute_loss(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        y, log_det_J = self.forward(x)\n",
    "        log_likelihood = self.distribution.log_pdf(y) + log_det_J\n",
    "        loss = -log_likelihood.sum()\n",
    "        return loss\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self.compute_loss(batch, batch_idx)\n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self.compute_loss(batch, batch_idx)\n",
    "        self.log('val_loss', loss)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss = self.compute_loss(batch, batch_idx)\n",
    "        self.log('test_loss', loss)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd099f04",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = NICE(distribution=StandardLogisticDistribution(device=DEVICE))\n",
    "model = model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f08976",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "trainer_kwargs = {\n",
    "    'gpus': 1,\n",
    "    'max_epochs': 5,\n",
    "    'precision': 16,\n",
    "    'progress_bar_refresh_rate': 5,\n",
    "    'weights_summary': \"full\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bc5b84",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(**trainer_kwargs)\n",
    "trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a7f22d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model.eval().to(DEVICE)\n",
    "nb_data = 10\n",
    "fig, axs = plt.subplots(nb_data, nb_data, figsize=(10, 10))\n",
    "logistic_distribution = StandardLogisticDistribution(device=DEVICE)\n",
    "for i in range(nb_data):\n",
    "    for j in range(nb_data):\n",
    "        x = model.invert(logistic_distribution.sample().unsqueeze(0)).data.cpu().numpy()\n",
    "        axs[i, j].imshow(x.reshape(28, 28).clip(0, 1), cmap='gray')\n",
    "        axs[i, j].set_xticks([])\n",
    "        axs[i, j].set_yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1887a0",
   "metadata": {},
   "source": [
    "# Task (6/6 points)\n",
    "\n",
    "Make RealNVP. Recap:\n",
    "\n",
    "* There are two neural networks: $s$ for scaling and $t$ for shifting;\n",
    "\n",
    "* Forward:\n",
    "\n",
    "$$ y^A = x^A \\times \\exp(s(x^B)) + t(x^B) $$\n",
    "\n",
    "$$ y^B = x^B $$\n",
    "\n",
    "* Jacobian determinant:\n",
    "\n",
    "$$ \\det(J) = \\exp \\Big( \\sum_{j=1:k}^d s(x^B)_j \\Big) $$\n",
    "\n",
    "* Inverse:\n",
    "\n",
    "$$ x^A = f^{-1}(y^A) = (y^A - t(x^B)) \\times \\exp(-s(x^B)) $$\n",
    "\n",
    "$$ x^B = f^{-1}(y^B) = y^B $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f716f451",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RealNVP(NICE):\n",
    "    def __init__(self, distribution, data_dim=28 * 28, hidden_dim=1000, n_transformations=4):\n",
    "        super().__init__(distribution)\n",
    "\n",
    "        self.distribution = distribution\n",
    "        self.n_transformations = n_transformations\n",
    "\n",
    "        # NN-transformations\n",
    "        self.t = torch.nn.ModuleList([nn.Sequential(\n",
    "            nn.Linear(data_dim // 2, hidden_dim), nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim), nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim), nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim), nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, data_dim // 2)\n",
    "        ) for i in range(n_transformations)])\n",
    "        self.s = torch.nn.ModuleList([nn.Sequential(\n",
    "            nn.Linear(data_dim // 2, hidden_dim), nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim), nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim), nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim), nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, data_dim // 2), nn.Tanh()\n",
    "        ) for i in range(n_transformations)])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        _x = x.clone()\n",
    "        log_det_J = x.new_zeros(x.shape[0])\n",
    "        \n",
    "        # normalizing flow\n",
    "        for i in range(len(self.s)):\n",
    "            \n",
    "            # split data\n",
    "            idxs_a = self.idxs_even if (i % 2 == 0) else self.idxs_odd\n",
    "            idxs_b = self.idxs_odd if (i % 2 == 0) else self.idxs_even\n",
    "            _x_a = _x[:, idxs_a]\n",
    "            _x_b = _x[:, idxs_b]\n",
    "            \n",
    "            # here is our formula from the lecture\n",
    "            s = <<your code here>>\n",
    "            t = <<your code here>>\n",
    "            _y_a = <<your code here>>\n",
    "            _y_b = <<your code here>>\n",
    "            log_det_J += torch.sum(s, dim=1) # log(exp(x)) = x, so we use just sum\n",
    "            \n",
    "            # trainsformation output\n",
    "            _x = torch.empty(_x.shape, device=_x.device)\n",
    "            _x[:, idxs_a] = _y_a\n",
    "            _x[:, idxs_b] = _y_b\n",
    "            \n",
    "        # flow output\n",
    "        y = _x\n",
    "            \n",
    "        return y, log_det_J\n",
    "\n",
    "    def invert(self, y):\n",
    "        \n",
    "        _y = y.clone()\n",
    "        \n",
    "        # inversion\n",
    "        for i in range(len(self.s) - 1, -1, -1):\n",
    "            \n",
    "            # split data\n",
    "            idxs_a = self.idxs_even if (i % 2) == 0 else self.idxs_odd\n",
    "            idxs_b = self.idxs_odd if (i % 2) == 0 else self.idxs_even\n",
    "\n",
    "            _y_a = _y[:, idxs_a]\n",
    "            _y_b = _y[:, idxs_b]\n",
    "            \n",
    "            # here is our formula from the lecture\n",
    "            s = <<your code here>>\n",
    "            t = <<your code here>>\n",
    "            _x_a = <<your code here>>\n",
    "            _x_b = <<your code here>>\n",
    "            \n",
    "            # make output\n",
    "            _y = torch.empty(y.shape, device=y.device)\n",
    "            _y[:, idxs_a] = _x_a\n",
    "            _y[:, idxs_b] = _x_b\n",
    "            \n",
    "        # inverse output\n",
    "        x = _y\n",
    "            \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ceac77",
   "metadata": {},
   "outputs": [],
   "source": [
    "your_model = RealNVP(distribution=StandardLogisticDistribution(device=DEVICE))\n",
    "your_model = your_model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab81a95",
   "metadata": {},
   "source": [
    "As we discussed in the lecture, normalizing flows are bijective transformations. Let us check this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c487a639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "y = logistic_distribution.sample().unsqueeze(0)\n",
    "x = your_model.invert(y)\n",
    "y_reconstructed = your_model.forward(x)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f298cf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.allclose(y, y_reconstructed, atol=1e-02)\n",
    "print(\"you seem to have written the right code\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6742d3b",
   "metadata": {},
   "source": [
    "Let's train this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fa1724",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(**trainer_kwargs)\n",
    "trainer.fit(your_model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01df99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# another test\n",
    "test_loss = trainer.test(your_model, val_loader)[0]['test_loss']\n",
    "assert test_loss < -10000\n",
    "print(\"it looks even more like you wrote the right code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4557c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "your_model.eval().to(DEVICE)\n",
    "nb_data = 10\n",
    "fig, axs = plt.subplots(nb_data, nb_data, figsize=(10, 10))\n",
    "logistic_distribution = StandardLogisticDistribution(device=DEVICE)\n",
    "for i in range(nb_data):\n",
    "    for j in range(nb_data):\n",
    "        x = your_model.invert(logistic_distribution.sample().unsqueeze(0)).data.cpu().numpy()\n",
    "        axs[i, j].imshow(x.reshape(28, 28).clip(0, 1), cmap='gray')\n",
    "        axs[i, j].set_xticks([])\n",
    "        axs[i, j].set_yticks([])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
