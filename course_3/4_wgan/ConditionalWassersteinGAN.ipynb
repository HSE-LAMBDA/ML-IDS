{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ConditionalWassersteinGAN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN6beUnH8ULddcTN8CKCXTV"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Wasserstein GAN and conditional GAN (2 in 1)"
      ],
      "metadata": {
        "id": "8On50hZyrOBT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This lab borrows a lot from the previous topic  notebook (Vanilla GAN). Again, we are going to train a GAN that mimics the images from the MNIST dataset. Again, most of the code is written for you already, and your task will be to fill in some of the missing parts.\n",
        "\n",
        "This time, though, we are going to change the GAN objective to turn our \"vanilla\" GAN into a **Wasserstein GAN** (WGAN-GP, [arXiv:1704.00028](https://arxiv.org/abs/1704.00028)). Another change is that this time we will train a **conditional GAN**: we will condition our generative model on the labels of the digits. In other words, we'll train our GAN to generate digits of a given sort."
      ],
      "metadata": {
        "id": "qb4UyKb0rbgK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As always, let's start by importing all the required libraries:"
      ],
      "metadata": {
        "id": "NKL_gx4gsOdI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ns3ONetC92pI"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import multiprocessing\n",
        "from IPython.display import clear_output"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PyTorch Lightning\n",
        "try:\n",
        "    import pytorch_lightning as pl\n",
        "except ModuleNotFoundError: # Google Colab does not have PyTorch Lightning\n",
        "                            # installed by default.\n",
        "                            # Hence, we do it here if necessary\n",
        "    !pip install pytorch-lightning==1.3.4\n",
        "    import pytorch_lightning as pl"
      ],
      "metadata": {
        "id": "--FwqdFj_HcH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And download the MNIST dataset:"
      ],
      "metadata": {
        "id": "wLpo49xKsVuF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.ToTensor()\n",
        "ds_train = MNIST(\".\", train=True, download=True, transform=transform)\n",
        "ds_test = MNIST(\".\", train=False, download=True, transform=transform)"
      ],
      "metadata": {
        "id": "bfuxcTyy_Nly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's a helper function to glue together a bunch of images and plot them in a matrix:"
      ],
      "metadata": {
        "id": "AYHfE6dcsaaE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def join_images(images):\n",
        "    # We expect `images` to be a 2D array of 2D images represented\n",
        "    # as uint8 numbers (from 0 to 255)\n",
        "    images = np.array(images)\n",
        "    assert images.ndim == 4, \"This function expects the input \" \\\n",
        "                             \"to be a 2D matrix of 2D images\"\n",
        "\n",
        "    # Let's pad our images with the white color to separate the\n",
        "    # nearby images once we glue them together\n",
        "    joined_image = np.pad(\n",
        "        images, ((0, 0), (0, 0), (0, 1), (0, 1)), constant_values=255\n",
        "    )\n",
        "\n",
        "    # Here we transpose and reshape our 4D array to look at it as if\n",
        "    # it was a single 2D image with all the sub-images placed in a\n",
        "    # rectangular grid\n",
        "    joined_image = np.transpose(joined_image, (0, 2, 1, 3))\n",
        "    joined_image = joined_image.reshape(\n",
        "        joined_image.shape[0] * joined_image.shape[1],\n",
        "        joined_image.shape[2] * joined_image.shape[3],\n",
        "    )\n",
        "\n",
        "    # Finally, we pad the top and left sides of our resulting image with\n",
        "    # the white color\n",
        "    joined_image = np.pad(\n",
        "        joined_image, ((1, 0), (1, 0)), constant_values=255\n",
        "    )\n",
        "\n",
        "    return joined_image\n",
        "\n",
        "# A simple function to quickly plot a matrix of images\n",
        "def plot_images(images):\n",
        "    plt.imshow(join_images(images), cmap=\"Greys\")\n",
        "    plt.axis(\"off\")"
      ],
      "metadata": {
        "id": "pjD9wAkW_epM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's test our functions:"
      ],
      "metadata": {
        "id": "DpnbqnPPt1XE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_images(\n",
        "    ds_train.data.numpy()[:20].reshape(4, 5, 28, 28)\n",
        ")"
      ],
      "metadata": {
        "id": "p5NHxIfzEPes"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OK, we've reached the interesting part! Similarly to the previous notebook, we are going to code all the logic of our GAN training within a single lightning module. Before we do, here's a quick reminder on how WGAN-GP works.\n",
        "\n",
        "Contrarily to the \"vanilla\" GAN, where the discriminator minimizes the binary cross-entropy loss for classifying its inputs into \"real\" and \"generated\" ones, the WGAN discriminator (or *critic*, as it's called in the original paper) solves the optimization task from the dual form of the Wasserstein distance between the real and generated distributions:\n",
        "$$W(p_{\\text{real}},p_{\\text{gen},\\theta})=\\sup_{\\Vert D\\Vert_L\\leq1}\\left[\\underset{x\\sim p_{\\text{data}}}{\\mathbb{E}}D(x) - \\underset{x\\sim p_{\\text{gen},\\theta}}{\\mathbb{E}}D(x)\\right],$$\n",
        "where $\\Vert D\\Vert_L\\leq1$ means that our discriminator is Lipschitz-continuous with the Lipschitz constant of 1. The latter property will be enforced by the gradient penalty term:\n",
        "$$\\lambda\\cdot\\underset{\\hat x\\,\\sim\\,p_{\\hat x}}{\\mathbb{E}}\\left[\\left(\\left\\Vert\\nabla_{\\hat x}D(\\hat x)\\right\\Vert-1\\right)^2\\right],$$\n",
        "$$\\hat x=\\alpha\\cdot x+(1-\\alpha)\\cdot y,$$\n",
        "$$x\\sim p_{\\text{data}},~~y\\sim p_{\\text{gen},\\theta},~~\\alpha\\sim\\text{Uniform}(0,1).$$"
      ],
      "metadata": {
        "id": "HPe4VHE6YAd1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "All the expectations in the formulas above are estimated on samples, i.e. the actuall loss functions are calculated as follows:\n",
        "$$L_D=L_D^{\\text{main}}+\\lambda\\cdot\\text{GP},$$\n",
        "$$L_D^{\\text{main}}=\\frac{1}{\\text{batch_size}}\\sum_i\\left[D(G(z_i))-D(x_i)\\right],$$\n",
        "$$\\text{GP}=\\frac{1}{\\text{batch_size}}\\sum_i\\left[\\left(\\left\\Vert\\nabla_{\\hat x_i}D(\\hat x_i)\\right\\Vert-1\\right)^2\\right],$$\n",
        "$$\\hat x_i\\,=\\,\\alpha_i\\ x_i\\,+\\,(1-\\alpha_i) G(z_i),$$\n",
        "$$L_G=\\frac{1}{\\text{batch_size}}\\sum_i\\left[-D(G(z_i))\\right].$$\n",
        "Here, $x_i$, $z_i$ and $\\alpha_i$ are samples from the real data, the latent and the Uniform(0, 1) distributions, respectively, and $\\Vert\\cdot\\Vert$ denotes the usual L2 norm of the gradient vector. As always, $L_D$ and $L_G$ are minimized in turns, with respect to the parameters of the discriminator and generator, respectively."
      ],
      "metadata": {
        "id": "eTI2V-0GcQLw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, since we want to train a *conditional* GAN, we need to add the conditional labels to both generator and discriminator networks at each call, i.e.:\n",
        "$$G(z_i)\\to G(z_i,c_i),$$\n",
        "$$D(x_i)\\to D(x_i,c_i),$$\n",
        "where $c_i$ is the label that corresponds to the object $x_i$ or to the object generated by the latent code $z_i$."
      ],
      "metadata": {
        "id": "gc-6Vx5HinDp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ok, let's code the model we've just described:"
      ],
      "metadata": {
        "id": "ukHAwlTqmXPn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GAN(pl.LightningModule):\n",
        "    def __init__(\n",
        "        self,\n",
        "        generator, # the generator network (torch.Module)\n",
        "        discriminator, # the discriminator network (torch.Module)\n",
        "        latent_size, # the dimensionality of z\n",
        "        num_disc_steps=1, # number of discriminator updates per single generator update\n",
        "        initial_lr=0.001, # initial learning rate\n",
        "        lr_decay_rate=0.95, # a factor, by which the learning rates will be multiplied after each epoch\n",
        "        gp_factor=10, # gradient penalty constant\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self.generator = generator\n",
        "        self.discriminator = discriminator\n",
        "        self.latent_size = latent_size\n",
        "        self.num_disc_steps = num_disc_steps\n",
        "        self.initial_lr = initial_lr\n",
        "        self.lr_decay_rate = lr_decay_rate\n",
        "        self.gp_factor = gp_factor\n",
        "\n",
        "        # We'll use this counter to switch between generator and discriminator steps\n",
        "        self._gan_step_counter = 0\n",
        "\n",
        "        # Important: This property activates manual optimization in lightning.\n",
        "        # Since GAN training steps are quite non-standard, we opt for manual\n",
        "        # optimization that we'll implement on our own.\n",
        "        self.automatic_optimization = False\n",
        "\n",
        "    # A function to sample a batch of latent vectors z\n",
        "    def generate_z(self, N):\n",
        "        return torch.randn(N, self.latent_size).to(self.device)\n",
        "\n",
        "    # A function to sample a batch of generated objects G(z, c)\n",
        "    def generate(self, one_hot_labels):\n",
        "        return self.generator(\n",
        "            self.generate_z(len(one_hot_labels)),\n",
        "            one_hot_labels,\n",
        "        )\n",
        "\n",
        "    # We'll use the function below to calculate both generator's and\n",
        "    # discriminator's losses\n",
        "    def _shared_losses_calculation(self, real_img_batch, one_hot_labels):\n",
        "        \"\"\"Calculate the loss value on a given batch\"\"\"\n",
        "\n",
        "        batch_size = len(real_img_batch)\n",
        "\n",
        "        # generate a batch of fakes:\n",
        "        fake_img_batch = self.generate(one_hot_labels)\n",
        "\n",
        "        # calculate the discriminator output on real and fake batches:\n",
        "        d_real = self.discriminator(real_img_batch, one_hot_labels)\n",
        "        d_fake = self.discriminator(fake_img_batch, one_hot_labels)\n",
        "\n",
        "        # Here we calculate the losses using the functions:\n",
        "        # `wgan_discriminator_loss_main`, `gradient_penalty` and\n",
        "        # `wgan_generator_loss` that you will implement later.\n",
        "        d_loss = (\n",
        "            wgan_discriminator_loss_main(d_real, d_fake)\n",
        "            + gradient_penalty(\n",
        "                batch_real=real_img_batch,\n",
        "                batch_fake=fake_img_batch,\n",
        "                one_hot_labels=one_hot_labels,\n",
        "                discriminator=self.discriminator,\n",
        "            ) * self.gp_factor\n",
        "        )\n",
        "        g_loss = wgan_generator_loss(d_fake)\n",
        "\n",
        "        return d_loss, g_loss\n",
        "\n",
        "    # This function will be iteratively called by lightning, automatically.\n",
        "    # Here we make our optimization steps.\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        # extract the objects, ignore the MNIST labels (digit indicies):\n",
        "        batch, labels = batch\n",
        "        one_hot_labels = torch.nn.functional.one_hot(labels, 10)\n",
        "\n",
        "        # get the optimizers (see the `configure_optimizers` method below):\n",
        "        d_opt, g_opt = self.optimizers()\n",
        "\n",
        "        # calculate both losses:\n",
        "        d_loss, g_loss = (\n",
        "            self._shared_losses_calculation(batch, one_hot_labels)\n",
        "        )\n",
        "\n",
        "        # Choose, which update step to make:\n",
        "        if self._gan_step_counter < self.num_disc_steps:\n",
        "            # Making a discriminator step\n",
        "            self._gan_step_counter += 1\n",
        "            d_opt.zero_grad()\n",
        "            self.manual_backward(d_loss) # https://pytorch-lightning.readthedocs.io/en/stable/common/lightning_module.html#manual-backward\n",
        "            d_opt.step()\n",
        "        else:\n",
        "            # Making a generator step\n",
        "            self._gan_step_counter = 0\n",
        "            g_opt.zero_grad()\n",
        "            self.manual_backward(g_loss) # https://pytorch-lightning.readthedocs.io/en/stable/common/lightning_module.html#manual-backward\n",
        "            g_opt.step()\n",
        "\n",
        "        # Logging our losses\n",
        "        self.log(\"train_loss_discriminator\", d_loss)\n",
        "        self.log(\"train_loss_generator\", g_loss)\n",
        "\n",
        "    # This function will be automatically called by lightning\n",
        "    # at each training epoch end. Inside, we are going to schedule our\n",
        "    # learning rates (see the `configure_optimizers` method below).\n",
        "    def training_epoch_end(self, outputs):\n",
        "        for sch in self.lr_schedulers():\n",
        "            sch.step()\n",
        "\n",
        "\n",
        "    # Here, we configure our optimizers for the discriminator and generator,\n",
        "    # along with learning rate schedulers.\n",
        "    def configure_optimizers(self):\n",
        "        d_opt = torch.optim.RMSprop(self.discriminator.parameters(), lr=self.initial_lr)\n",
        "        g_opt = torch.optim.RMSprop(self.generator.parameters(), lr=self.initial_lr)\n",
        "\n",
        "        # A learning rate scheduler is an object that changes the learning rate\n",
        "        # during training. Below, we create ExponentialLR instances to\n",
        "        # exponentially decay the learning rates (multiply the learning rate by\n",
        "        # a factor of 0.95 after each epoch).\n",
        "        d_scheduler = torch.optim.lr_scheduler.ExponentialLR(d_opt, self.lr_decay_rate)\n",
        "        g_scheduler = torch.optim.lr_scheduler.ExponentialLR(g_opt, self.lr_decay_rate)\n",
        "\n",
        "        return [\n",
        "            {\"optimizer\" : d_opt, \"lr_scheduler\" : d_scheduler},\n",
        "            {\"optimizer\" : g_opt, \"lr_scheduler\" : g_scheduler},\n",
        "        ]\n"
      ],
      "metadata": {
        "id": "pP8QXlCEEkh-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task (6 points)\n",
        "Fill the blanks below to implement the functions:\n",
        "1. `wgan_discriminator_loss_main` (2 points),\n",
        "2. `wgan_generator_loss` (2 points),\n",
        "3. `gradient_penalty` (2 points).\n",
        "\n",
        "For the reference, see the definitions of $L_D^{\\text{main}}$, $L_G$ and $\\text{GP}$."
      ],
      "metadata": {
        "id": "jjQOZEefrOoq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def wgan_discriminator_loss_main(d_real, d_fake):\n",
        "    \"\"\"\n",
        "    Calculates loss for the discriminator (not including the gradient penalty term).\n",
        "    Input parameters:\n",
        "      d_real -- discriminator outputs for a batch of real objects and their\n",
        "                respective labels\n",
        "      d_fake -- discriminator outputs for a batch of fake objects and their\n",
        "                respective labels\n",
        "    \"\"\"\n",
        "    raise NotImplementedError # <= remove this\n",
        "    # <YOUR CODE>\n",
        "    # return <YOUR CODE>\n",
        "\n",
        "\n",
        "def wgan_generator_loss(d_fake):\n",
        "    \"\"\"\n",
        "    Calculates loss for the generator.\n",
        "    Input parameters:\n",
        "      d_fake -- discriminator outputs for a batch of fake objects and their\n",
        "                respective labels\n",
        "    \"\"\"\n",
        "    raise NotImplementedError # <= remove this\n",
        "    # <YOUR CODE>\n",
        "    # return <YOUR CODE>\n",
        "\n",
        "\n",
        "def gradient_penalty(batch_real, batch_fake, one_hot_labels, discriminator):\n",
        "    \"\"\"\n",
        "    Calculates the gradient penalty term.\n",
        "    Input parameters:\n",
        "      batch_real -- a batch of real objects\n",
        "      batch_fake -- a batch of fake objects (should be of same size as\n",
        "                    `batch_real`)\n",
        "      one_hot_labels -- a batch of one-hot labels, corresponding to the objects\n",
        "                        in `batch_real` and `batch_fake`\n",
        "      discriminator -- the discriminator network (torch.Module)\n",
        "    \"\"\"\n",
        "    # First, let's calculate the x_hat from the GP formula, i.e. the linear\n",
        "    # interpolates between the real and fake objects.\n",
        "    \n",
        "    # We'll sample the interpolation coefficients `alpha` from a uniform\n",
        "    # distribution. We need to be careful, though, to get a single `alpha` value\n",
        "    # for each image in the batch (i.e., we want to have the same alpha sample\n",
        "    # for all the pixels of the same image, but different alpha samples for\n",
        "    # different images). Since our images are of the shape (B, C, H, W),\n",
        "    # this can be acheved by sampling `alpha` of the shape (B, 1, 1, 1).\n",
        "    batch_size = len(batch_real)\n",
        "    alpha = torch.empty(\n",
        "        batch_size, 1, 1, 1, dtype=batch_real.dtype, device=batch_real.device\n",
        "    )\n",
        "    alpha.uniform_(0, 1)\n",
        "    interpolates = alpha * batch_real + (1 - alpha) * batch_fake\n",
        "\n",
        "    # Now, let's calculate the D(x_hat):\n",
        "    d_output = discriminator(interpolates, one_hot_labels)\n",
        "\n",
        "    # Now we can calculate the gradients. We'll use the `torch.autograd.grad`\n",
        "    # function to do that. It's important to pass `create_graph=True` to it,\n",
        "    # such that we can then backpropagate through the result when optimizing\n",
        "    # our discriminator. Check out the docs:\n",
        "    #   https://pytorch.org/docs/stable/generated/torch.autograd.grad.html\n",
        "    \n",
        "    # One of the important arguments here is the one called `grad_outputs`.\n",
        "    # It can typically be ignored when calculating the gradient of a scalar,\n",
        "    # but now we are calculating the gradient of a vector (`d_output` is a batch\n",
        "    # of the discriminator outputs). Hence, this argument is mandatory.\n",
        "    \n",
        "    # A gradient of a vector with respect to another vector is a matrix\n",
        "    # (Jacobian). The `torch.autograd.grad` function is implemented to return\n",
        "    # `matmul(Jacobian, grad_outputs)` product, i.e. the matrix-vector product\n",
        "    # of the Jacobian matrix and the `grad_outputs` vector.\n",
        "\n",
        "    # Think, what vector should be passed to the `grad_outputs` argument to\n",
        "    # get the correct result, and fill the gap below:\n",
        "\n",
        "    # grad_outputs = <YOUR CODE>\n",
        "    raise NotImplementedError # <= remove this\n",
        "\n",
        "    # And now we calulate the gradient. The resulting gradient has the same\n",
        "    # shape as our images, so we reshape it to (B, C * H * W) to treat it as a\n",
        "    # vector later.\n",
        "    grads = torch.autograd.grad(\n",
        "        [d_output],\n",
        "        [interpolates],\n",
        "        create_graph=True,\n",
        "        grad_outputs=grad_outputs,\n",
        "    )[0].view(batch_size, -1)\n",
        "\n",
        "    # OK, now that you have the `grads` vector (dD(x_hat)/dx_hat), use it to\n",
        "    # calculate the gradient penalty value.\n",
        "    # HINT: use `torch.linalg.norm` to calculate the norm. Calculating it\n",
        "    # manually by squaring, summing and then taking the sqrt may result in a NaN\n",
        "    # for a derivative if the input vector has 0 length (due to the 0 / 0\n",
        "    # ambiguity).\n",
        "\n",
        "    # gradient_penalty_value = <YOUR CODE>\n",
        "    raise NotImplementedError # <= remove this\n",
        "\n",
        "    return gradient_penalty_value"
      ],
      "metadata": {
        "id": "ULQv39zxpDWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You may use the cell below for self-control (a correct solution should not trigger any of the assertions)."
      ],
      "metadata": {
        "id": "1u5-kyNkW5yc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_solution():\n",
        "    dummy_d_real = torch.tensor(\n",
        "        [[-1.3130], [ 0.2434], [ 0.9388], [-0.3919], [-1.1801], [-0.9238], [-0.3502], [-0.7253], [ 0.2569], [ 1.4235]]\n",
        "    )\n",
        "    dummy_d_fake = torch.tensor(\n",
        "        [[-0.0455], [-1.0809], [ 0.7711], [-0.4530], [-0.1560], [ 1.3168], [ 0.8033], [-0.0238], [-0.5631], [-0.0213]]\n",
        "    )\n",
        "    dummy_batch_real = torch.tensor(\n",
        "        [[[[ 0.6031, -1.3677, -0.6469,  1.3742], [-0.5418,  1.9298,  0.7391,  1.9177], [-1.4565, -1.2624, -0.5140,  0.9718], [-0.3452, -0.8339,  0.6958,  1.6030]]],\n",
        "         [[[ 0.2721, -0.1995, -1.0205,  0.5386], [-0.4417, -1.2073, -0.4647, -0.3979], [ 0.0356, -0.6618, -0.4126, -0.0598], [ 1.1912,  2.4061, -0.0862, -0.3650]]],\n",
        "         [[[ 1.6891, -0.3839,  0.1758, -2.6211], [ 0.8489,  1.3923,  1.7145,  1.2908], [-1.3113,  1.4497, -1.4642,  1.1934], [ 0.5669, -0.9430,  0.7101, -1.6577]]],\n",
        "         [[[ 0.3040,  0.0942, -0.5878, -0.4074], [ 0.9896,  0.0596,  0.7263, -0.9611], [ 1.3899, -0.7076, -1.7282, -0.4437], [ 0.4054, -0.1272,  0.9978,  0.3708]]],\n",
        "         [[[ 0.5256,  0.7467,  0.3299, -0.3802], [-0.6723, -0.9191,  0.3278,  1.1507], [-0.0032,  0.7865, -0.2089, -0.7193], [ 0.3707, -2.0075, -1.1139,  0.1478]]]]\n",
        "    )\n",
        "    dummy_batch_fake = torch.tensor(\n",
        "        [[[[-2.1661, -0.2350,  0.8560, -0.2495], [-0.4252,  1.8806, -1.4996, -0.8637], [-0.9359,  1.2126, -0.3936, -0.4491], [ 0.5070,  2.5815, -0.6126, -1.8609]]],\n",
        "         [[[-1.4590, -0.4350, -0.5206,  0.1118], [-0.6213,  1.0411,  0.1881, -1.2227], [-1.3277,  0.4584,  0.3738, -1.3614], [ 0.9316, -1.9789,  0.0483, -0.6561]]],\n",
        "         [[[ 1.8826,  1.0135, -1.2973, -1.8827], [-0.8091, -0.7376,  0.7225,  0.7375], [-0.8654,  1.5320,  0.8099,  0.2867], [-0.7880, -0.3300,  0.0338, -1.9983]]],\n",
        "         [[[-0.3476,  0.2015, -1.0666, -0.5320], [-0.9068,  1.1607,  0.7148,  0.1309], [ 1.5926,  1.7366, -0.0851,  0.8583], [ 1.0615, -1.7650, -0.2727,  0.2254]]],\n",
        "         [[[ 1.0907, -0.2203,  0.8887,  0.1553], [-0.4685, -0.9549, -2.0325, -0.6273], [ 1.0975,  0.2569,  0.1859,  0.9267], [ 0.9180, -0.6636,  0.4972,  1.5740]]]]\n",
        "    )\n",
        "    dummy_batch_real.requires_grad = True\n",
        "    dummy_labels = torch.nn.functional.one_hot(torch.arange(5), 10)\n",
        "    def dummy_discriminator(x, labels):\n",
        "        dummy_multiplier_1 = torch.tensor(\n",
        "            [[[[-1.0278, -1.0076, -0.9431,  0.8558],\n",
        "               [ 1.4569,  1.1453,  0.0753,  0.3377],\n",
        "               [ 0.9183,  1.7088, -0.1621,  0.1573],\n",
        "               [ 0.6179, -0.9540,  0.2306, -0.2688]]]]\n",
        "        )\n",
        "        dummy_multiplier_2 = torch.tensor(\n",
        "            [[-0.4931,  0.8827, -0.1299,  0.3261, -0.2990, -0.0691, -1.5011,  0.2234, 0.3917,  1.4503]]\n",
        "        )\n",
        "        return (\n",
        "            (x * dummy_multiplier_1).sum(axis=(2, 3))\n",
        "            + (labels * dummy_multiplier_2).sum(axis=1, keepdims=True)\n",
        "        )\n",
        "\n",
        "    d_loss_main = wgan_discriminator_loss_main(dummy_d_real, dummy_d_fake)\n",
        "    g_loss = wgan_generator_loss(dummy_d_fake)\n",
        "    gp_value = gradient_penalty(\n",
        "        dummy_batch_real, dummy_batch_fake, dummy_labels, dummy_discriminator\n",
        "    )\n",
        "\n",
        "    d_loss_main_reference = torch.tensor(0.25693002343177795)\n",
        "    g_loss_reference = torch.tensor(-0.054760001599788666)\n",
        "    gp_value_reference = torch.tensor(6.413931846618652)\n",
        "\n",
        "    assert d_loss_main.numel() == 1, \"`wgan_discriminator_loss_main` should \" \\\n",
        "        \"return a single number. Did you forget to average the result?\"\n",
        "    assert torch.isclose(\n",
        "        d_loss_main, d_loss_main_reference\n",
        "    ).item(), \"Failed test for `wgan_discriminator_loss_main`: expected \" \\\n",
        "        f\"{d_loss_main_reference.item()}, got {d_loss_main.item()}\"\n",
        "    assert g_loss.numel() == 1, \"`wgan_generator_loss` should \" \\\n",
        "        \"return a single number. Did you forget to average the result?\"\n",
        "    assert torch.isclose(\n",
        "        g_loss, g_loss_reference\n",
        "    ).item(), \"Failed test for `wgan_generator_loss`: expected \" \\\n",
        "        f\"{g_loss_reference.item()}, got {g_loss.item()}\"\n",
        "    assert gp_value.numel() == 1, \"`gradient_penalty` should \" \\\n",
        "        \"return a single number. Did you forget to average the result?\"\n",
        "    assert torch.isclose(\n",
        "        gp_value, gp_value_reference\n",
        "    ).item(), \"Failed test for `gradient_penalty`: expected \" \\\n",
        "        f\"{gp_value_reference.item()}, got {gp_value.item()}\"\n",
        "\n",
        "    print(\"All tests passed!\")\n",
        "\n",
        "test_solution()"
      ],
      "metadata": {
        "id": "Kwp4YFnk_LxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code cell below defines the generator and discriminator architectures. Since we work with images, we decide to follow the regular deep convolutional network architecture for the discriminator. For the generator, we interleave convolutions and transposed convolutions with residual connections to upsample the images.\n",
        "\n",
        "Compared to the previous notebook, we reduced the numbers of channels to make our networks more lightweight. This is mainly due to the gradient penalty term, which makes everything run considerably slower.\n",
        "\n",
        "Another modification is the conditional labels. In the generator, we just concatenate them with the latent code $z$. For the discriminator, we decide to make all the convolutional part not conditioned and only concatenate the condition to the dense representation afterwards. Note that we use the one-hot representation of the labels (therefore they are held in 10-dimensional vectors)."
      ],
      "metadata": {
        "id": "6twc_YoHHP0f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This will be our upsampling block with a residual connection:\n",
        "# we'll upsample the input in two ways and then add the results together.\n",
        "# The two upsampling ways are:\n",
        "#   1) with a transposed convolution with a trainable kernel\n",
        "#   2) with a fixed nearest-neighbor interpolation upsampling\n",
        "class UpsampleWithRes(torch.nn.Module):\n",
        "    def __init__(self, upconv, activation, factor, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self.path_a = torch.nn.Sequential(\n",
        "            upconv, activation()\n",
        "        )\n",
        "        self.factor = factor\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_a = self.path_a(x)\n",
        "        x_b = torch.nn.functional.interpolate(x, scale_factor=self.factor)\n",
        "        return x_a + x_b\n",
        "\n",
        "# Define the generator architecture\n",
        "class ConvGenerator(torch.nn.Module):\n",
        "    def __init__(self, activation, latent_size, labels_size=10, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self.fc = torch.nn.Sequential(\n",
        "            torch.nn.Linear(latent_size + labels_size, 256),\n",
        "            activation(),\n",
        "        )\n",
        "        self.backbone = torch.nn.Sequential( # 8x8\n",
        "            torch.nn.Conv2d(\n",
        "                in_channels=4, out_channels=64, kernel_size=3, padding=1,\n",
        "            ), # -> 8x8\n",
        "            activation(),\n",
        "            UpsampleWithRes(\n",
        "                upconv=torch.nn.ConvTranspose2d(\n",
        "                    in_channels=64, out_channels=64, kernel_size=4, stride=4, padding=0,\n",
        "                ),\n",
        "                activation=activation,\n",
        "                factor=4,\n",
        "            ), # -> 32x32\n",
        "            torch.nn.Conv2d(\n",
        "                in_channels=64, out_channels=32, kernel_size=3, padding=0,\n",
        "            ), # -> 30x30\n",
        "            activation(),\n",
        "            torch.nn.Conv2d(\n",
        "                in_channels=32, out_channels=16, kernel_size=3, padding=0,\n",
        "            ), # -> 28x28\n",
        "            activation(),\n",
        "            torch.nn.Conv2d(\n",
        "                in_channels=16, out_channels=1, kernel_size=1, padding=0,\n",
        "            ),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, labels):\n",
        "        return self.backbone(\n",
        "            self.fc(\n",
        "                torch.cat([x, labels], axis=1)\n",
        "            ).view(-1, 4, 8, 8)\n",
        "        )\n",
        "\n",
        "\n",
        "# Define the discriminator architecture\n",
        "class ConvDiscriminator(torch.nn.Module):\n",
        "    def __init__(self, activation, labels_size=10, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self.backbone = torch.nn.Sequential( # 28x28\n",
        "            torch.nn.Conv2d(\n",
        "                in_channels=1, out_channels=16, kernel_size=3, padding=1,\n",
        "            ), # -> 28x28\n",
        "            activation(),\n",
        "            torch.nn.Conv2d(\n",
        "                in_channels=16, out_channels=32, kernel_size=4, stride=4, padding=0,\n",
        "            ), # -> 7x7\n",
        "            activation(),\n",
        "            torch.nn.Conv2d(\n",
        "                in_channels=32, out_channels=32, kernel_size=3, padding=0,\n",
        "            ), # -> 5x5\n",
        "            activation(),\n",
        "            torch.nn.Conv2d(\n",
        "                in_channels=32, out_channels=32, kernel_size=3, padding=0,\n",
        "            ), # -> 3x3\n",
        "            activation(),\n",
        "        )\n",
        "        self.fc = torch.nn.Sequential(\n",
        "            torch.nn.Linear(9 * 32 + labels_size, 32),\n",
        "            activation(),\n",
        "            torch.nn.Linear(32, 1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, labels):\n",
        "        conv_features = self.backbone(x).view(-1, 9 * 32)\n",
        "        return self.fc(\n",
        "            torch.cat([conv_features, labels], axis=1)\n",
        "        )"
      ],
      "metadata": {
        "id": "ZITbkvPsM9D3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we'll create a callback to plot some generator images at the end of each training epoch. We'll store the generated images in tensorboard."
      ],
      "metadata": {
        "id": "d9QVo9-yIyVO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PlotDigitsCallback(pl.Callback):\n",
        "    def on_train_epoch_end(self, trainer, module):\n",
        "        tensorboard = module.logger.experiment\n",
        "        image = join_images(\n",
        "            (\n",
        "                module.generate(\n",
        "                    torch.nn.functional.one_hot(\n",
        "                        torch.arange(30) % 10, 10\n",
        "                    ).to(module.device)\n",
        "                ).detach().cpu().numpy().clip(0, 1).reshape(\n",
        "                    5, 6, 28, 28\n",
        "                ) * 255\n",
        "            ).astype(\"uint8\")\n",
        "        )\n",
        "        tensorboard.add_image(\n",
        "            \"generated_images\",\n",
        "            image,\n",
        "            trainer.global_step, dataformats=\"HW\"\n",
        "        )"
      ],
      "metadata": {
        "id": "BEK1KPDLqzts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, let's create our networks, the GAN module and a lightning trainer object:"
      ],
      "metadata": {
        "id": "Uneo6CnoJCRh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LATENT_SIZE = 128\n",
        "model = GAN(\n",
        "    generator=ConvGenerator(\n",
        "        activation=torch.nn.ELU, latent_size=LATENT_SIZE\n",
        "    ),\n",
        "    discriminator=ConvDiscriminator(\n",
        "        activation=torch.nn.ELU,\n",
        "    ),\n",
        "    latent_size=LATENT_SIZE,\n",
        "    num_disc_steps=3,\n",
        "    initial_lr=0.001,\n",
        ")\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    ds_train,\n",
        "    batch_size=100,\n",
        "    shuffle=True,\n",
        "    num_workers=multiprocessing.cpu_count(),\n",
        ")\n",
        "\n",
        "trainer = pl.Trainer(\n",
        "    gpus=1,\n",
        "    max_epochs=20,\n",
        "    log_every_n_steps=5,\n",
        "    flush_logs_every_n_steps=10,\n",
        "    callbacks=[\n",
        "        PlotDigitsCallback(),\n",
        "        pl.callbacks.LearningRateMonitor(),\n",
        "    ],\n",
        ")"
      ],
      "metadata": {
        "id": "Iva4jETfaN_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize tensorboard to monitor progress."
      ],
      "metadata": {
        "id": "SBglmJrYJTgs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Start tensorboard.\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir lightning_logs/"
      ],
      "metadata": {
        "id": "mUb8cSiGdcjk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we may train our model. While it's training, keep an eye on the tensorboard interface above. You can monitor the losses in the \"SCALARS\" tab, but you should also be able to look at the images produced by the generator in the \"IMAGES\" tab."
      ],
      "metadata": {
        "id": "6icnbx1RJY_Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.fit(model, train_loader)"
      ],
      "metadata": {
        "id": "X1GmFr8Sd1Sr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's plot some images that our generator produces:"
      ],
      "metadata": {
        "id": "inONVk6nJxDV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_images(\n",
        "    (\n",
        "        model.generate(\n",
        "            torch.nn.functional.one_hot(\n",
        "                torch.arange(100) % 10, 10\n",
        "            ).to(model.device)\n",
        "        ).detach().cpu().numpy().clip(0, 1).reshape(10, 10, 28, 28) * 255\n",
        "    ).astype(\"uint8\")\n",
        ")"
      ],
      "metadata": {
        "id": "CGq7L3aRd6g7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "sDzqFA4TsWZq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}