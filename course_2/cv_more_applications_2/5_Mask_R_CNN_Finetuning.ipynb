{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ukh1XBgJClXK"
   },
   "source": [
    "# Mask R-CNN for instance segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "37TzVfH_CsfG"
   },
   "source": [
    "This notebook is heavily based on [this tutorial](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html). The main goal here is to take a Mask R-CNN model pre-trained on the COCO dataset and fine-tune it on the [Penn-Fudan Database for Pedestrian Detection and Segmentation](https://www.cis.upenn.edu/~jshi/ped_html/). Let start with downloading the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZnVKXEFA04Jv"
   },
   "outputs": [],
   "source": [
    "!wget https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip\n",
    "!unzip PennFudanPed.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DIjmjuvM0XwP"
   },
   "source": [
    "We will rely on torchvision heavily. E.g., we'll take its pre-trained Mask R-CNN model, and we'll also re-use some of the object-detection code. To get the correct version of the code, check out the installed version of torchvision:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jk9yG_IiBAGk"
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "print(torchvision.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V1OG7won1GA8"
   },
   "source": [
    "As of time of preparing this notebook, the torchvision version installed in Colab is 0.10.0. So that's the tag we'll use to download the required source files (make sure to find the tag matching your version of torchvision):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TnhxnV6YCVnv"
   },
   "outputs": [],
   "source": [
    "TORCHVISION_TAG = \"v0.10.0\"\n",
    "!wget https://raw.githubusercontent.com/pytorch/vision/{TORCHVISION_TAG}/references/detection/transforms.py\n",
    "!wget https://raw.githubusercontent.com/pytorch/vision/{TORCHVISION_TAG}/references/detection/utils.py\n",
    "!wget https://raw.githubusercontent.com/pytorch/vision/{TORCHVISION_TAG}/references/detection/engine.py\n",
    "!wget https://raw.githubusercontent.com/pytorch/vision/{TORCHVISION_TAG}/references/detection/coco_utils.py\n",
    "!wget https://raw.githubusercontent.com/pytorch/vision/{TORCHVISION_TAG}/references/detection/coco_eval.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v7faVTn91i3o"
   },
   "source": [
    "Let's create a torch dataset for the Penn-Fudan database that we downloaded earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ATr3Y-WxDU-F"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "class PennFudanDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, transforms):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        # load all image files, sorting them to\n",
    "        # ensure that they are aligned\n",
    "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"PNGImages\"))))\n",
    "        self.masks = list(sorted(os.listdir(os.path.join(root, \"PedMasks\"))))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # load images and masks\n",
    "        img_path = os.path.join(self.root, \"PNGImages\", self.imgs[idx])\n",
    "        mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        # note that we haven't converted the mask to RGB,\n",
    "        # because each color corresponds to a different instance\n",
    "        # with 0 being background\n",
    "        mask = Image.open(mask_path)\n",
    "        # convert the PIL Image into a numpy array\n",
    "        mask = np.array(mask)\n",
    "        # instances are encoded as different colors\n",
    "        obj_ids = np.unique(mask)\n",
    "        # first id is the background, so remove it\n",
    "        obj_ids = obj_ids[1:]\n",
    "\n",
    "        # split the color-encoded mask into a set\n",
    "        # of binary masks\n",
    "        masks = mask == obj_ids[:, None, None]\n",
    "\n",
    "        # get bounding box coordinates for each mask\n",
    "        num_objs = len(obj_ids)\n",
    "        boxes = []\n",
    "        for i in range(num_objs):\n",
    "            pos = np.where(masks[i])\n",
    "            xmin = np.min(pos[1])\n",
    "            xmax = np.max(pos[1])\n",
    "            ymin = np.min(pos[0])\n",
    "            ymax = np.max(pos[0])\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "        # convert everything into a torch.Tensor\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        # there is only one class\n",
    "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
    "        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
    "\n",
    "        image_id = torch.tensor([idx])\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        # suppose all instances are not crowd\n",
    "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"masks\"] = masks\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SpNbX0Qs1z5L"
   },
   "source": [
    "Now we'll create an instance of the newly defined dataset and have a look at some of its examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "68uApeM3EE1k"
   },
   "outputs": [],
   "source": [
    "import transforms as T\n",
    "\n",
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    transforms.append(T.ToTensor())\n",
    "    if train:\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "    return T.Compose(transforms)\n",
    "\n",
    "dataset = PennFudanDataset('PennFudanPed', get_transform(train=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aaLk6iRcEcG8"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(9, 6), dpi=100)\n",
    "\n",
    "img_ids = (0, 1, 2)\n",
    "for i_img in img_ids:\n",
    "    example_img, example_target = dataset[i_img]\n",
    "    example_masks = example_target[\"masks\"]\n",
    "    example_masks = example_masks + torch.where(\n",
    "        example_masks,\n",
    "        torch.arange(1, example_masks.shape[0] + 1, dtype=example_masks.dtype)[:,None,None],\n",
    "        torch.zeros_like(example_masks),\n",
    "    )\n",
    "    example_masks = example_masks.sum(axis=0)\n",
    "\n",
    "    plt.subplot(2, len(img_ids), i_img + 1)\n",
    "    plt.imshow(example_img.permute(1, 2, 0))\n",
    "    plt.axis(\"off\")\n",
    "    for bbox in example_target[\"boxes\"]:\n",
    "        plt.plot(\n",
    "            [bbox[0], bbox[2], bbox[2], bbox[0], bbox[0]],\n",
    "            [bbox[1], bbox[1], bbox[3], bbox[3], bbox[1]],\n",
    "            linewidth=2.0\n",
    "        )\n",
    "    plt.subplot(2, len(img_ids), i_img + len(img_ids) + 1)\n",
    "    plt.imshow(example_masks)\n",
    "    plt.axis(\"off\");\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mFee0LMREIFg"
   },
   "source": [
    "Now that we have our dataset, let's take a pre-trained Mask R-CNN from torchvision and see, what its predictions look like on the Penn-Fudan data.\n",
    "\n",
    "*Note: we're predicting for the `example_img` created in the last iteration of the `for` loop from the previous cell.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P93FnuUdCbrJ"
   },
   "outputs": [],
   "source": [
    "pretrained_maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
    "pretrained_maskrcnn.eval()\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "with torch.autograd.no_grad():\n",
    "    predictions = pretrained_maskrcnn.to(device)([example_img.to(device)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jrlOMsL6FE_U"
   },
   "source": [
    "Let's superimpose the prediction masks with the original image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kana1ZH2DF-y"
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "\n",
    "plt.figure(figsize=(7, 7), dpi=100)\n",
    "plt.imshow(example_img.permute(1, 2, 0));\n",
    "plt.axis(\"off\");\n",
    "\n",
    "cycler = iter(plt.rcParams[\"axes.prop_cycle\"])\n",
    "mapped_colors = {}\n",
    "\n",
    "for mask, category in zip(\n",
    "    predictions[0][\"masks\"].squeeze(1).cpu().numpy(),\n",
    "    predictions[0][\"labels\"].cpu().numpy()\n",
    "):\n",
    "    if category in mapped_colors:\n",
    "        color = mapped_colors[category]\n",
    "    else:\n",
    "        color = next(cycler)[\"color\"]\n",
    "        mapped_colors[category] = color\n",
    "\n",
    "    plt.imshow(\n",
    "        np.ma.masked_array(mask, mask=(mask < 0.5)),\n",
    "        alpha=0.5,\n",
    "        cmap=matplotlib.colors.ListedColormap([color])\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "82fWV8VtFLU1"
   },
   "source": [
    "Nice results! It seems like the model is highlighting a lot of stuff, though, while we only need to mark the pedestrians. In fact, we can just pick the 'person' category from the pre-trained model to achieve that. But that's not exciting enough... Instead, let's fine-tune the model on our data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hOf8xkOnGPJy"
   },
   "source": [
    "To do that, we'll get the pre-trained Mask R-CNN, chop some of its heads off and replace with brand new ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YDJ-GTB4LsY3"
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "def get_model_instance_segmentation(num_classes):\n",
    "    # load an instance segmentation model pre-trained on COCO\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "    # get number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # now get the number of input features for the mask classifier\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    # and replace the mask predictor with a new one\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n",
    "                                                       hidden_layer,\n",
    "                                                       num_classes)\n",
    "\n",
    "    return model\n",
    "\n",
    "# Two classes (background and pedestrian).\n",
    "test_model = get_model_instance_segmentation(num_classes=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HXziwbm8HKO4"
   },
   "source": [
    "Let's check, whether the model is compatible with the dataset. First, we create a data loader and get a single batch of data from it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "raEM_sMERlye"
   },
   "outputs": [],
   "source": [
    "import utils\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=2, shuffle=True, num_workers=4,\n",
    "    collate_fn=utils.collate_fn     # Since different images in the batch may be\n",
    "                                    # of different spatial sizes, one cannot\n",
    "                                    # stack them into a single tensor. This\n",
    "                                    # function will do the job of putting them\n",
    "                                    # in a list of tensors. Same applies to the\n",
    "                                    # ground truth masks.\n",
    ")\n",
    "images, targets = next(iter(data_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OczPMvaAHfdR"
   },
   "source": [
    "Then, we make a forward pass through the model. Note that the model is implemented to behave differently in the `train` and `eval` modes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mk55QEZ7SbY5"
   },
   "outputs": [],
   "source": [
    "# When in training mode, the model expects both images and targets,\n",
    "# and returns a dictionary containing loss values.\n",
    "\n",
    "test_model.train()\n",
    "losses = test_model(images, targets)\n",
    "\n",
    "for k, v in losses.items():\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZxbIRbiPS737"
   },
   "outputs": [],
   "source": [
    "# When in test mode, the model takes the input images only and\n",
    "# returns the list of prediction dictionaries:\n",
    "\n",
    "test_model.eval()\n",
    "outputs = test_model(images)\n",
    "for out in outputs:\n",
    "    print(out.keys())\n",
    "    for k, v in out.items():\n",
    "        print(\"\\t\", k, v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mlxywnfJJfdi"
   },
   "source": [
    "Seems like everything works as intended. Now we can run the training process.\n",
    "\n",
    "*Note: running for 10 epochs may take about 20 minutes on collab. It should be ok to run for fewer epochs (at least 2-3) to finish the exercises from this notebook.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kp5LlG29qMOd"
   },
   "outputs": [],
   "source": [
    "from engine import train_one_epoch, evaluate\n",
    "\n",
    "# use our dataset and defined transformations\n",
    "dataset = PennFudanDataset('PennFudanPed', get_transform(train=True))\n",
    "dataset_test = PennFudanDataset('PennFudanPed', get_transform(train=False))\n",
    "\n",
    "# split the dataset in train and test set\n",
    "indices = torch.randperm(len(dataset)).tolist()\n",
    "dataset = torch.utils.data.Subset(dataset, indices[:-50])\n",
    "dataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n",
    "\n",
    "# define training and validation data loaders\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=2, shuffle=True, num_workers=2,\n",
    "    collate_fn=utils.collate_fn\n",
    ")\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "    dataset_test, batch_size=1, shuffle=False, num_workers=2,\n",
    "    collate_fn=utils.collate_fn\n",
    ")\n",
    "\n",
    "# get the model using our helper function\n",
    "model = get_model_instance_segmentation(num_classes=2)\n",
    "\n",
    "# move model to the right device\n",
    "model.to(device)\n",
    "\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "                            momentum=0.9, weight_decay=0.0005)\n",
    "# and a learning rate scheduler\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer, step_size=3, gamma=0.1\n",
    ")\n",
    "\n",
    "# let's train it for 10 epochs\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # train for one epoch, printing every 10 iterations\n",
    "    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
    "    # update the learning rate\n",
    "    lr_scheduler.step()\n",
    "    # evaluate on the test dataset\n",
    "    evaluate(model, data_loader_test, device=device)\n",
    "\n",
    "print(\"That's it!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dWsYKg3wSSix"
   },
   "source": [
    "Now, we'll write a function to make a prediction on a single image and plot the original image and the image superimposed with the predicted masks, side by side:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c66xyf5bn_AR"
   },
   "outputs": [],
   "source": [
    "def eval_on_img(model, image):\n",
    "    model.eval()\n",
    "    with torch.autograd.no_grad():\n",
    "        pred, = model([image.to(device)])\n",
    "\n",
    "    pred_mask = pred[\"masks\"].cpu().numpy().squeeze(1)\n",
    "    for i in range(pred_mask.shape[0]):\n",
    "        pred_mask[i] = np.where(pred_mask[i] > 0.5, i + 1, 0)\n",
    "    pred_mask = pred_mask.max(axis=0)\n",
    "    pred_mask = np.ma.masked_array(pred_mask, mask=(pred_mask == 0))\n",
    "\n",
    "    plt.figure(figsize=(9, 6))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(image.permute(1, 2, 0));\n",
    "    plt.axis(\"off\");\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(image.permute(1, 2, 0));\n",
    "    plt.imshow(pred_mask, alpha=0.9);\n",
    "    plt.axis(\"off\");\n",
    "\n",
    "    plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m7YbCtebwP0o"
   },
   "outputs": [],
   "source": [
    "test_img, _ = next(iter(dataset_test))\n",
    "eval_on_img(model, test_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VEa3oPWoTKZa"
   },
   "source": [
    "Let's see what our model would predict on an image outside of our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QQ67Sqoely72"
   },
   "outputs": [],
   "source": [
    "!wget https://upload.wikimedia.org/wikipedia/en/1/18/Inception_OST.jpg\n",
    "\n",
    "external_img = Image.open(\"Inception_OST.jpg\").convert(\"RGB\")\n",
    "external_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jQzDdTTlCChs"
   },
   "source": [
    "## Task 1 (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mRHlqaQVTWUb"
   },
   "source": [
    "Now it's your turn to apply our model on this image. Use the `eval_on_img` function defined above. Don't forget to convert `external_img` to a torch tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fs1ZeaY2n4EU"
   },
   "outputs": [],
   "source": [
    "# <YOUR CODE>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lIcwEgfpCFdo"
   },
   "source": [
    "## Task 2 (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tzUpJhnNUiAG"
   },
   "source": [
    "**Final quest**\n",
    "\n",
    "This last task intends you to dig a little into the insides of the model. The overall goal here is to plot the map of the RPNs attention (objectness scores) to the input image (`external_img`).\n",
    "\n",
    "To find out how to do this, check out what's happening in our model's [forward call](https://github.com/pytorch/vision/blob/c50d0fccd5b0b47dea162b4dac2fd94ae21e522f/torchvision/models/detection/generalized_rcnn.py#L42) (for simplicity, let's only consider the `eval` mode, i.e., `self.training = False` and `targets = None`).\n",
    "\n",
    "As you should know from the theoretical material, the RPN (Region Proposal Network) is a separate head that acts on the convolutional features obtained by the main fully convolutional backbone of the model. Try to locate where this happens in the code. Note that RPN's forward call [already returns the proposed regions](https://github.com/pytorch/vision/blob/c50d0fccd5b0b47dea162b4dac2fd94ae21e522f/torchvision/models/detection/generalized_rcnn.py#L96), rather than the objectness scores, so we'll need to go deeper into the code to get the RPN's objectness scores. That is, we need to look inside [the RPN's forward method](https://github.com/pytorch/vision/blob/c50d0fccd5b0b47dea162b4dac2fd94ae21e522f/torchvision/models/detection/rpn.py#L319). Note, by the way, that the RPN is tructured to have its own separate head for [predicting the objectness scores](https://github.com/pytorch/vision/blob/c50d0fccd5b0b47dea162b4dac2fd94ae21e522f/torchvision/models/detection/rpn.py#L343).\n",
    "\n",
    "From the docstring and the annotations, we can see that the convolutional features (the `features` argument) is expected to be a dictionary. The reason for that is that RPN looks at different scales of the convolutional features and makes its predictions for each scale. So, this dictionary has a separate entry for each of the scales. Since, by design, RPN predicts the objectness scores (and corresponding bounding boxes) for anchors centered at **each of the pixels in the convolutional feature map**, the output of the RPN is going to be a set of image-like objectess maps. These are the maps that we want to plot. Each map is going to have multiple channels - one for each of the anchors in a given pixel. You can plot these channels separetly, or, alternatively, you can take the maximum objectess score for each of the pixels.\n",
    "\n",
    "So, to sum up, you need to do the following things:\n",
    "- [transform the input image](https://github.com/pytorch/vision/blob/c50d0fccd5b0b47dea162b4dac2fd94ae21e522f/torchvision/models/detection/generalized_rcnn.py#L77), (1 point)\n",
    "- [calculate the convolutional features](https://github.com/pytorch/vision/blob/c50d0fccd5b0b47dea162b4dac2fd94ae21e522f/torchvision/models/detection/generalized_rcnn.py#L93), (1 point)\n",
    "- [calculate the RPN's objectness score maps](https://github.com/pytorch/vision/blob/c50d0fccd5b0b47dea162b4dac2fd94ae21e522f/torchvision/models/detection/rpn.py#L343), (1 point)\n",
    "- \\[optionally\\] get the maximum value at each pixel of each of the objectness score maps, (1 point)\n",
    "- plot each of the resulting maps using `plt.imshow`. (1 point)\n",
    "\n",
    "Try to visually compare the original `external_img` input image with the obtained RPN's attention maps. Which areas of the image got the most attention?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OGPS7cCvpYxn"
   },
   "outputs": [],
   "source": [
    "# For reference, see the `forward` methods code for the main model and its RPN:\n",
    "#     https://github.com/pytorch/vision/blob/c50d0fccd5b0b47dea162b4dac2fd94ae21e522f/torchvision/models/detection/generalized_rcnn.py#L42\n",
    "#     https://github.com/pytorch/vision/blob/c50d0fccd5b0b47dea162b4dac2fd94ae21e522f/torchvision/models/detection/rpn.py#L319\n",
    "\n",
    "def get_rpm_output(model, image):\n",
    "    model.eval()\n",
    "    with torch.autograd.no_grad():\n",
    "        imgs = [image]\n",
    "        # <YOUR CODE>\n",
    "\n",
    "    # `proposal_scores` should be a list of rpn's proposal score maps\n",
    "    return proposal_scores\n",
    "\n",
    "proposals = get_rpm_output(model, T.F.to_tensor(external_img).to(device))\n",
    "\n",
    "# Feel free to adjust this part for a nicer plot:\n",
    "plt.figure(figsize=(30, 10))\n",
    "for i, prop in enumerate(proposals, 1):\n",
    "    plt.subplot(1, len(proposals), i)\n",
    "    plt.imshow(prop.cpu(), interpolation=\"nearest\")\n",
    "    plt.axis(\"off\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bKVSdKlH-L56"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "5_Mask_R-CNN_Finetuning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
