{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sounds classification\n",
    "\n",
    "In this notebook we will consider urban sounds classification using CNNs.\n",
    "\n",
    "The notebook is based on this [notebook](https://github.com/hse-ds/iad-applied-ds/blob/master/2021/hw/hw2/HW2.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation\n",
    "\n",
    "Firstly, we need to install several python libraries we will use in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch==1.8.0 torchaudio==0.8.0 numpy==1.20.0 pytorch-lightning==1.5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "\n",
    "import torchaudio\n",
    "\n",
    "device='cuda:0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will classify audio files from [UrbanSound8K](https://urbansounddataset.weebly.com/urbansound8k.html) dataset.\n",
    "\n",
    "It contains 8732 records of various urban sounds, that are splitted on train/val/test subsamples.\n",
    "\n",
    "![image](https://paperswithcode.com/media/datasets/UrbanSound8K-0000003722-02faef06.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get install unzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# download the data\n",
    "!rm -r ./data\n",
    "!mkdir ./data/\n",
    "!pip install gdown\n",
    "!cd ./data && gdown https://drive.google.com/uc?id=1XY-jjMs_CQ9ZSD0lMecUSIYKkRI0BG2s && unzip -qq HW2_dataset.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains the following 10 defferent classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sound classes\n",
    "classes = ['air_conditioner', 'car_horn', 'children_playing', 'dog_bark',\n",
    "           'drilling', 'engine_idling', 'gun_shot', 'jackhammer', \n",
    "           'siren', 'street_music']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the following class for the dataset. It will read audio records from files and resturn objects `(x, y, len)`, where `x` is a waveform data of a record, `y` is a class label and `len` is length of the record. All waveforms will have the same size of `pad_size`. If a record is shorter than `pad_size`, it will be filled with zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_RATE = 44100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, path_to_csv, \n",
    "                 path_to_folder, \n",
    "                 pad_size=384000,\n",
    "                 sr=SAMPLE_RATE):\n",
    "        \n",
    "        self.csv = pd.read_csv(path_to_csv)[['ID', 'Class']]\n",
    "        self.path_to_folder = path_to_folder\n",
    "        self.pad_size = pad_size\n",
    "        self.sr = sr\n",
    "        \n",
    "        self.class_to_idx = {classes[i]: i for i in range(10)}\n",
    "\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        item = self.csv.iloc[index]\n",
    "        wav_idx, cl = item\n",
    "\n",
    "        path = os.path.join(self.path_to_folder, f'{wav_idx}.wav')\n",
    "        wav, sr = torchaudio.load(path, format='wav')\n",
    "        wav = wav[0]\n",
    "        len = wav.shape[0]\n",
    "\n",
    "        wav_padded = torch.zeros(self.pad_size)\n",
    "        wav_padded[:len] = wav\n",
    "\n",
    "        instance = {\n",
    "            'x': wav_padded,\n",
    "            'y': self.class_to_idx[cl],\n",
    "            'len': len\n",
    "        }\n",
    "\n",
    "        return instance\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.csv.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create train and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = AudioDataset('./data/urbansound8k/train_part.csv', './data/urbansound8k/data')\n",
    "val_dataset = AudioDataset('./data/urbansound8k/val_part.csv', './data/urbansound8k/data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the first record\n",
    "item = train_dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the waveform\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.plot(item['x'])\n",
    "plt.title(item['y'])\n",
    "\n",
    "# play the record\n",
    "display.Audio(item['x'], rate=SAMPLE_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dataloaders, that we will used to fit our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, 32, shuffle=True, pin_memory=True, drop_last=True)\n",
    "val_dataloader = DataLoader(val_dataset, 32, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mel spectrogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use mel spectrograms of the records as inputs for our model. Let's define a function, that transforms waveforms into log mel spectrograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchaudio.transforms import MelSpectrogram\n",
    "\n",
    "def compute_log_melspectrogram(wav_batch, lens, device='cpu'):\n",
    "    \n",
    "    featurizer = MelSpectrogram(\n",
    "                            sample_rate=SAMPLE_RATE,\n",
    "                            n_fft=1024,\n",
    "                            win_length=1024,\n",
    "                            hop_length=256,\n",
    "                            n_mels=64,\n",
    "                            center=False,\n",
    "                            ).to(device)\n",
    "\n",
    "    return torch.log(featurizer(wav_batch).clamp(1e-5)), lens // 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a batch of records\n",
    "for batch in train_dataloader:\n",
    "    break\n",
    "\n",
    "wav_batch = batch['x']\n",
    "y_batch = batch['y']\n",
    "lens = batch['len']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get log of mel spectrograms\n",
    "log_melspect, lens = compute_log_melspectrogram(wav_batch, lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the result\n",
    "plt.figure(figsize=(16, 8))\n",
    "\n",
    "for i in range(5):\n",
    "    plt.subplot(5, 1, i+1)\n",
    "    plt.imshow(log_melspect[i].numpy())\n",
    "    plt.title(f'Reference log melspectorgram {i}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "\n",
    "Implement the convolutional neural network CNN10 from the paper [PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition](https://arxiv.org/pdf/1912.10211.pdf):\n",
    "\n",
    "* 2x (Conv2d 3x3 @ 16, BN, ReLU)\n",
    "* MaxPoll 2x2\n",
    "* 2x (Conv2d 3x3 @ 32, BN, ReLU)\n",
    "* MaxPoll 2x2\n",
    "* 2x (Conv2d 3x3 @ 64, BN, ReLU)\n",
    "* MaxPoll 2x2\n",
    "* (Conv2d 3x3 @ 128, BN, ReLU)\n",
    "* (Conv2d 2x2 @ 128, BN, ReLU)\n",
    "* Global MaxPoll\n",
    "* Fully Connected 128, ReLU\n",
    "* Fully Connected 10\n",
    "\n",
    "**Hint:** use functions `nn.Conv2d()`, `nn.BatchNorm2d()` and `nn.ReLU()`. For mel spectrograms use `compute_log_melspectrogram` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch import nn\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "class Model(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, num_classes=10, hidden=16):\n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "        self.cnn = nn.Sequential(\n",
    "            \n",
    "            ### BEGIN SOLUTION\n",
    "            \n",
    "            ### END SOLUTION\n",
    "        )\n",
    "\n",
    "        \n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(8 * hidden, 8 * hidden), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8 * hidden, num_classes)\n",
    "        )\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z = self.cnn(x[:, None, :, :])\n",
    "        z = torch.nn.functional.max_pool2d(z, kernel_size=z.size()[2:])[:, :, 0, 0]\n",
    "        z = self.mlp(z)\n",
    "        return z\n",
    "\n",
    "    \n",
    "    # calculate loss function values\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x = batch['x']\n",
    "        y = batch['y']\n",
    "        lens = batch['len']\n",
    "        \n",
    "        # get log mel spectrograms\n",
    "        ### BEGIN SOLUTION\n",
    "\n",
    "        ### BEGIN SOLUTION\n",
    "        \n",
    "        pred = self(x)\n",
    "        \n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        loss = criterion(pred, y)\n",
    "        return loss\n",
    "\n",
    "    \n",
    "    # define optimizer to fit the network\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit model\n",
    "\n",
    "Fit the model. It takes about 15 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# define trainer to fit out network\n",
    "trainer = pl.Trainer(max_epochs=10, gpus=1)\n",
    "\n",
    "# init our netwrok\n",
    "model = Model()\n",
    "\n",
    "# fit the netwrok\n",
    "trainer.fit(model, train_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you solve the task correctly, you will get accuracy on validation about `0.92`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(model, val_dataloader, device):\n",
    "    \n",
    "    pred_true_pairs = []\n",
    "    for batch in val_dataloader:\n",
    "        x = batch['x'].to(device)\n",
    "        y = batch['y'].to(device)\n",
    "        lens = batch['len'].to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            x, lens = compute_log_melspectrogram(x, lens, device=device)\n",
    "            probs = model(x)\n",
    "            pred_cls = probs.argmax(dim=-1)\n",
    "\n",
    "        for pred, true in zip(pred_cls.cpu().detach().numpy(), y.cpu().numpy()):\n",
    "            pred_true_pairs.append((pred, true))\n",
    "\n",
    "    print(f'Val accuracy: {np.mean([p[0] == p[1] for p in pred_true_pairs])}')\n",
    "\n",
    "    cm_df = pd.DataFrame(\n",
    "        confusion_matrix([p[1] for p in pred_true_pairs], \n",
    "                         [p[0] for p in pred_true_pairs], \n",
    "                         normalize='true'),\n",
    "        columns=classes, index=classes)\n",
    "    sn.heatmap(cm_df, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(model, val_dataloader, model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
