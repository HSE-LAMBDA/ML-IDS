{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8OvKUo8_E2jf"
   },
   "source": [
    "# [HIGGS UCI](https://archive.ics.uci.edu/ml/datasets/HIGGS#) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QzyjBbxzE2jm"
   },
   "source": [
    "In this notebook will work with real dataset from high energy physics (HEP). The goal is increase the accuracy on test subset compared to baseline. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Lz2Z3YIE2jm"
   },
   "source": [
    "### Introduction\n",
    "\n",
    "The field of high energy physics is devoted to the study of the elementary constituents of matter. By investigating the structure of matter and the laws that govern its interactions, this field strives to discover the fundamental\n",
    "properties of the physical universe. The primary tools of experimental high energy physicists are modern accelerators, which collide protons and/or antiprotons to create exotic particles that occur only at extremely high\n",
    "energy densities. Collisions at high energy particle colliders are a fruitful source of exotic particle discoveries. Observing these particles and measuring their properties may yield critical insights about the\n",
    "very nature of matter. Finding these rare particles requires solving difficult signal-versus-background classification problems.  Such discoveries require powerful statistical methods, and machine learning tools play\n",
    "a critical role. Given the limited quantity and expensive nature of the data, improvements in analytical tools\n",
    "directly boost particle discovery potential."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HcsvunTDE2jo"
   },
   "source": [
    "### Data Set Information:\n",
    "\n",
    "The data has been produced using Monte Carlo simulations. \n",
    "\n",
    "The first column is the class label:\n",
    "- 1 for signal, \n",
    "- 0 for background.\n",
    "\n",
    "Other 28 columns are features (21 low-level features then 7 high-level features): \n",
    "- lepton pT, \n",
    "- lepton eta, \n",
    "- lepton phi, \n",
    "- missing energy magnitude, \n",
    "- missing energy phi, \n",
    "- jet 1 pt, \n",
    "- jet 1 eta, \n",
    "- jet 1 phi, \n",
    "- jet 1 b-tag, \n",
    "- jet 2 pt, \n",
    "- jet 2 eta, \n",
    "- jet 2 phi, \n",
    "- jet 2 b-tag, \n",
    "- jet 3 pt, \n",
    "- jet 3 eta, \n",
    "- jet 3 phi, \n",
    "- jet 3 b-tag, \n",
    "- jet 4 pt, \n",
    "- jet 4 eta, \n",
    "- jet 4 phi, \n",
    "- jet 4 b-tag, \n",
    "\n",
    "\n",
    "- m_jj, \n",
    "- m_jjj, \n",
    "- m_lv, \n",
    "- m_jlv, \n",
    "- m_bb, \n",
    "- m_wbb, \n",
    "- m_wwbb.\n",
    "\n",
    "The first 21 features (columns 1-21) are kinematic properties measured by the particle detectors in the accelerator. The last seven features are functions of the first 21 features; these are high-level features derived by physicists to help discriminate between the two classes.\n",
    "\n",
    "The last 500,000 examples are used as a test set.\n",
    "\n",
    "Benchmark results using Bayesian Decision Trees from a standard physics package and 5-layer neural networks are presented in the original paper:\n",
    "\n",
    "**Baldi, P., P. Sadowski, and D. Whiteson.** “[Searching for Exotic Particles in High-energy Physics with Deep Learning](https://arxiv.org/pdf/1402.4735.pdf).” Nature Communications 5 (July 2, 2014).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jXbMDJTbE2jp"
   },
   "outputs": [],
   "source": [
    "# import packages\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import wget\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# apply pip install for those you don`t have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "unjI-_I6FDMZ"
   },
   "outputs": [],
   "source": [
    "# pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "nKJWZoUPE2ju",
    "outputId": "d82245d2-947a-460c-a646-ae64e343cca4"
   },
   "outputs": [],
   "source": [
    "# donwload the HIGGS dataset\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00280/HIGGS.csv.gz\"  \n",
    "wget.download(url, 'HIGGS.csv.gz') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q3hkMuzWE2ju"
   },
   "outputs": [],
   "source": [
    "# open the dataset \n",
    "\n",
    "data = pd.read_csv('HIGGS.csv.gz', names=list(range(29)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZBshxbetE2jv",
    "outputId": "89477369-b673-411f-91a2-e7ed6c007cb9"
   },
   "outputs": [],
   "source": [
    "print('Number of rows = ', data.shape[0])\n",
    "print('Number of columns = ', data.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MQPsKH2xE2jv"
   },
   "source": [
    "We will divide the data to train and test subsets. Our goal is to predict the class from the first column (with number 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ff5rQ25AFfN3"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cfm6yUekHD_b",
    "outputId": "38567d59-2728-4035-dcc2-a56ce3bb4cba"
   },
   "outputs": [],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QyWRhKz_OlYs"
   },
   "source": [
    "The last 500 000 of samples we will use as test subset to check the perfomance of our model. The remaining dataset has 10.5 millions of observations that will require a lot of time to train the model and tune hyperparameters. That is why we will reduce the number of training samples to 1 million. Since the dataset can be imbalanced we should not extract samples randomly. We will extract samples the way that train subset will have the same classes distribution as dataset using *stratify* in **train_test_split()**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oEgPRj0uOjoj"
   },
   "outputs": [],
   "source": [
    "# extract last 500 000 observations to test subset\n",
    "\n",
    "X_test, y_test = data.to_numpy()[-500000:,1:], data.to_numpy()[-500000:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sMoFcGPsHgaw"
   },
   "outputs": [],
   "source": [
    "# extract training set with the same distribution\n",
    "# 0.2/21 is choosen to obtain 100 thousand of observations\n",
    "\n",
    "X, y = data.to_numpy()[:-500000,1:], data.to_numpy()[:-500000,0]\n",
    "X_train, X_val, y_train, y_val = train_test_split(X,y, train_size=0.2/21, random_state=42, stratify=data.to_numpy()[:-500000,0],)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b04QqHHKR1Hp"
   },
   "source": [
    "Let us check if both distributions are realy coincide:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "_C-S2CfeI4J2",
    "outputId": "e3cb7bbe-aafc-4a80-af08-2249c7d6b063"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "fig = plt.figure(figsize=(8,4))\n",
    "plt.hist(y,density=True,rwidth=1,align='right',label='distribution of classes in y');\n",
    "plt.hist(y_train,density=True,rwidth=1,align='left',label='distribution of classes in y_train');\n",
    "plt.legend()\n",
    "plt.title('Classes distribution');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0NqRCOCCSDk1"
   },
   "source": [
    "Yes, they are! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eKJkq0bNE2jw"
   },
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M_V81I6tE2jw"
   },
   "source": [
    "Using the default values of hyperparametrs of [XGboost](https://xgboost.readthedocs.io/en/latest/parameter.html#) classifier we will train the model and check the accuracy on the test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**XGBoost** means Extreme Gradient Boosting. This is a specific implementation of the Gradient Boosting approach which uses more accurate approximations to find the best tree model. It has some tricks that make it exceptionally successful, such as computing second-order gradients of the loss function and advanced regularization (L1 and L2). XGBoost training fast and can be parallelized.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A4rSVFJ-E2jx"
   },
   "outputs": [],
   "source": [
    "# init the model\n",
    "\n",
    "clf_base = xgb.XGBClassifier(random_state=42, n_jobs=-1) #  n_jobs is an integer, specifying the maximum number of concurrently running workers\n",
    "                                                         #  n_jobs=-1 means all possible workers are going to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b5DIee9KE2jy",
    "outputId": "a126f141-a967-431d-c0b9-760a0e729d75"
   },
   "outputs": [],
   "source": [
    "# fit the model on the train subset\n",
    "%%time\n",
    "clf_base.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_a0vfvVk6ehA",
    "outputId": "59af8807-af39-440e-caa2-b1e2144d0202"
   },
   "outputs": [],
   "source": [
    "# make predictions on the test subset\n",
    "\n",
    "pred_base = clf_base.predict(X_test)\n",
    "\n",
    "print('Accuracy on the test is ', round(accuracy_score(pred_base, y_test),4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9hkYZ5DBb1FA"
   },
   "source": [
    "So, you may see that default values of hyperparameters gave us 71% of correctnes in our binary classes problem. Is it possible to increase the accuracy on the test?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZFxWQUqIE2j0"
   },
   "source": [
    "### Task:\n",
    "By tuning hyperparameters of the model, obtain the accuracy on the test greater or equal to 0.73. Do not forget to set random_state=42. What are the values of the hyperparameters for the best model?\n",
    "\n",
    "- learning rate\n",
    "- gamma\n",
    "- maximum depth\n",
    "- number of estimators\n",
    "- L1 regularization parameter\n",
    "- L2 regularization parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tsj9axmp7ghK"
   },
   "source": [
    "### Parameters to tune\n",
    "- **learning_rate** [default=0.3]\n",
    "\n",
    "  Parameter of the learning rate. Step size shrinkage used in update to prevents overfitting. After each boosting step, we can directly get the weights of new features, and eta shrinks the feature weights to make the boosting process more conservative. The range is [0,1].\n",
    "  \n",
    "\n",
    "- **gamma** [default=0]\n",
    "\n",
    "  Parameter of the min split loss. Minimum loss reduction required to make a further partition on a leaf node of the tree. The larger gamma is, the more conservative the algorithm will be. The range is [0,∞].\n",
    "  \n",
    "\n",
    "- **max_depth** [default=6]\n",
    "\n",
    "  Maximum depth of a tree. Increasing this value will make the model more complex and more likely to overfit. 0 is only accepted in lossguide growing policy when tree_method is set as hist or gpu_hist and it indicates no limit on depth. Beware that XGBoost aggressively consumes memory when training a deep tree. The range is [0,∞] \n",
    "\n",
    "\n",
    "- **n_estimators** [default=100]\n",
    "\n",
    "  Number of gradient boosted trees. Equivalent to number of boosting rounds.\n",
    "  \n",
    "  \n",
    "- **reg_lambda** [default=1]\n",
    "\n",
    "    L2 regularization term on weights. Increasing this value will make model more conservative.\n",
    "    \n",
    "\n",
    "- **reg_alpha** [default=0]\n",
    "\n",
    "    L1 regularization term on weights. Increasing this value will make model more conservative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OIWx9d-oZ_Za"
   },
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_1hMqziWbrIL"
   },
   "outputs": [],
   "source": [
    "# your code"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "HIGGS_UCI.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
