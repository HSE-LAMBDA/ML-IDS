{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [HIGGS UCI](https://archive.ics.uci.edu/ml/datasets/HIGGS#) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook will work with real dataset from high energy physics (HEP). The goal is increase the accuracy on test subset compared to baseline. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "The field of high energy physics is devoted to the study of the elementary constituents of matter. By investigating the structure of matter and the laws that govern its interactions, this field strives to discover the fundamental\n",
    "properties of the physical universe. The primary tools of experimental high energy physicists are modern accelerators, which collide protons and/or antiprotons to create exotic particles that occur only at extremely high\n",
    "energy densities. Collisions at high energy particle colliders are a fruitful source of exotic particle discoveries. Observing these particles and measuring their properties may yield critical insights about the\n",
    "very nature of matter. Finding these rare particles requires solving difficult signal-versus-background classification problems.  Such discoveries require powerful statistical methods, and machine learning tools play\n",
    "a critical role. Given the limited quantity and expensive nature of the data, improvements in analytical tools\n",
    "directly boost particle discovery potential."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Set Information:\n",
    "\n",
    "The data has been produced using Monte Carlo simulations. \n",
    "\n",
    "The first column is the class label:\n",
    "- 1 for signal, \n",
    "- 0 for background.\n",
    "\n",
    "Other 28 columns are features (21 low-level features then 7 high-level features): \n",
    "- lepton pT, \n",
    "- lepton eta, \n",
    "- lepton phi, \n",
    "- missing energy magnitude, \n",
    "- missing energy phi, \n",
    "- jet 1 pt, \n",
    "- jet 1 eta, \n",
    "- jet 1 phi, \n",
    "- jet 1 b-tag, \n",
    "- jet 2 pt, \n",
    "- jet 2 eta, \n",
    "- jet 2 phi, \n",
    "- jet 2 b-tag, \n",
    "- jet 3 pt, \n",
    "- jet 3 eta, \n",
    "- jet 3 phi, \n",
    "- jet 3 b-tag, \n",
    "- jet 4 pt, \n",
    "- jet 4 eta, \n",
    "- jet 4 phi, \n",
    "- jet 4 b-tag, \n",
    "\n",
    "\n",
    "- m_jj, \n",
    "- m_jjj, \n",
    "- m_lv, \n",
    "- m_jlv, \n",
    "- m_bb, \n",
    "- m_wbb, \n",
    "- m_wwbb.\n",
    "\n",
    "The first 21 features (columns 1-21) are kinematic properties measured by the particle detectors in the accelerator. The last seven features are functions of the first 21 features; these are high-level features derived by physicists to help discriminate between the two classes.\n",
    "\n",
    "The last 500,000 examples are used as a test set.\n",
    "\n",
    "Benchmark results using Bayesian Decision Trees from a standard physics package and 5-layer neural networks are presented in the original paper:\n",
    "\n",
    "**Baldi, P., P. Sadowski, and D. Whiteson.** “[Searching for Exotic Particles in High-energy Physics with Deep Learning](https://arxiv.org/pdf/1402.4735.pdf).” Nature Communications 5 (July 2, 2014).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import wget\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# apply pip install for those you don`t have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# donwload the HIGGS dataset\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00280/HIGGS.csv.gz\"  \n",
    "wget.download(url, 'HIGGS.csv.gz') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the dataset \n",
    "\n",
    "data = pd.read_csv('HIGGS.csv.gz', names=list(range(29)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of rows = ', data.shape[0])\n",
    "print('Number of columns = ', data.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will divide the data to train and test subsets. Our goal is to predict the class from the first column (with number 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = data.to_numpy()[:-500000,1:], data.to_numpy()[:-500000,0]\n",
    "X_test, y_test = data.to_numpy()[-500000:,1:], data.to_numpy()[-500000:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the default values of hyperparametrs of XGboost classifier we will train the model and check the accuracy on the test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init the model\n",
    "\n",
    "clf = xgb.XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model on the train subset\n",
    "\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions on the test subset\n",
    "\n",
    "pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy on the test is ', round(accuracy_score(pred, y_test),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** by applying any model that you wish and tuning its hyperparameters, increase the accuracy on the test."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azar_env",
   "language": "python",
   "name": "azar_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
