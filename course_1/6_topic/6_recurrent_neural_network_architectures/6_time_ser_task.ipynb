{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Processing\n",
    "\n",
    "Source:\n",
    "\n",
    "* https://www.kaggle.com/tartakovsky/pytorch-lightning-lstm-timeseries-clean-code\n",
    "* https://keras.io/examples/timeseries/timeseries_weather_forecasting/\n",
    "\n",
    "We will be using Jena Climate dataset recorded by the Max Planck Institute for Biogeochemistry. The dataset consists of 14 features such as temperature, pressure, humidity etc, recorded once per 10 minutes.\n",
    "\n",
    "Location: Weather Station, Max Planck Institute for Biogeochemistry in Jena, Germany\n",
    "\n",
    "Time-frame Considered: Jan 10, 2009 - December 31, 2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install pytorch-lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-loads all imports every time the cell is ran. \n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.display.float_format = '{:,.5f}'.format\n",
    "from IPython.display import display\n",
    "\n",
    "# Sklearn tools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Neural Networks\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "from pytorch_lightning.loggers.csv_logs import CSVLogger\n",
    "\n",
    "# Plotting\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "plt.style.use('bmh')\n",
    "mpl.rcParams['figure.figsize'] = 18, 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget -nc \"https://storage.googleapis.com/tensorflow/tf-keras-datasets/jena_climate_2009_2016.csv.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "zip_file = ZipFile(\"jena_climate_2009_2016.csv.zip\")\n",
    "zip_file.extractall()\n",
    "csv_path = \"jena_climate_2009_2016.csv\"\n",
    "\n",
    "data = pd.read_csv(csv_path)\n",
    "data = data.iloc[::200] # only each 200th point\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_time_key = \"Date Time\"\n",
    "data[date_time_key] = pd.to_datetime(data[date_time_key], infer_datetime_format=True)\n",
    "data.set_index(date_time_key, inplace=True)\n",
    "data.sort_index(inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "\n",
    "Let's see what data we got as a result, and also make visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "titles = [\n",
    "    \"Pressure\",\n",
    "    \"Temperature\",\n",
    "    \"Temperature in Kelvin\",\n",
    "    \"Temperature (dew point)\",\n",
    "    \"Relative Humidity\",\n",
    "    \"Saturation vapor pressure\",\n",
    "    \"Vapor pressure\",\n",
    "    \"Vapor pressure deficit\",\n",
    "    \"Specific humidity\",\n",
    "    \"Water vapor concentration\",\n",
    "    \"Airtight\",\n",
    "    \"Wind speed\",\n",
    "    \"Maximum wind speed\",\n",
    "    \"Wind direction in degrees\",\n",
    "]\n",
    "\n",
    "feature_keys = [\n",
    "    \"p (mbar)\",\n",
    "    \"T (degC)\",\n",
    "    \"Tpot (K)\",\n",
    "    \"Tdew (degC)\",\n",
    "    \"rh (%)\",\n",
    "    \"VPmax (mbar)\",\n",
    "    \"VPact (mbar)\",\n",
    "    \"VPdef (mbar)\",\n",
    "    \"sh (g/kg)\",\n",
    "    \"H2OC (mmol/mol)\",\n",
    "    \"rho (g/m**3)\",\n",
    "    \"wv (m/s)\",\n",
    "    \"max. wv (m/s)\",\n",
    "    \"wd (deg)\",\n",
    "]\n",
    "\n",
    "colors = [\n",
    "    \"blue\",\n",
    "    \"orange\",\n",
    "    \"green\",\n",
    "    \"red\",\n",
    "    \"purple\",\n",
    "    \"brown\",\n",
    "    \"pink\",\n",
    "    \"gray\",\n",
    "    \"olive\",\n",
    "    \"cyan\",\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "def show_raw_visualization(data):\n",
    "    time_data = data.index\n",
    "    fig, axes = plt.subplots(nrows=7, ncols=2, figsize=(15, 20), dpi=80, facecolor=\"w\", edgecolor=\"k\")\n",
    "    for i in range(len(feature_keys)):\n",
    "        key = feature_keys[i]\n",
    "        c = colors[i % (len(colors))]\n",
    "        t_data = data[key]\n",
    "        t_data.index = time_data\n",
    "        t_data.head()\n",
    "        ax = t_data.plot(ax=axes[i // 2, i % 2], color=c, title=\"{} - {}\".format(titles[i], key), rot=25,)\n",
    "        ax.legend([titles[i]])\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "show_raw_visualization(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose data\n",
    "\n",
    "Further experiments will be carried out with a Celsius temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name = \"T (degC)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series: play with data\n",
    "\n",
    "Let's take a look at how data can be decomposed into components:\n",
    "\n",
    "- **Trend**: The increasing or decreasing value in the series;\n",
    "- **Seasonality**: The repeating relatively short-term cycle in the series;\n",
    "- **Noise**: The random deviations in the series.\n",
    "\n",
    "And also draw the [autocorrelation graph](https://otexts.com/fpp2/non-seasonal-arima.html#acf-and-pacf-plots)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trend and seasonality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "decomposition = seasonal_decompose(data[data_name], model='additive', freq=365)\n",
    "decomposition.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autocorrelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import lag_plot\n",
    "\n",
    "import pandas as pd\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "\n",
    "autocorrelation_plot(data[data_name])\n",
    "plt.title(f'{data_name} autocorrelation.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('autocorrelation coef:', data[data_name].autocorr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "\n",
    "Plot the correlation of $y_{t}$ with $y_ {t+1}$ and calculate the correlation coefficient (see an example in the lecture). What conclusions can we draw from the results obtained?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_curr = # your code where\n",
    "y_next = # your code where\n",
    "\n",
    "# your code where (scatter plot)\n",
    "\n",
    "plt.title('Correlation of the previous value y(t) with the next one y(t+1).')\n",
    "plt.xlabel('y(t)')\n",
    "plt.ylabel('y(t+1)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrcoef = # your code where\n",
    "print('Correlation coefficient:', corrcoef)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: we see a rather strong positive correlation of the previous step with the next one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AR model for time series forecast\n",
    "\n",
    "\n",
    "Consider $y_1, y_2, ..., y_i, ..., y_N$ are observations of a time series. Autoregressive Model for the time series forecast assumes the following:\n",
    "\n",
    "$$ \\hat{y}_{i+m} = f( y_{i}, y_{i-1}, ... y_{i-k+1} ) $$\n",
    "\n",
    "where $\\hat{y}_{i+m}$ is a predicted value.\n",
    "\n",
    "In matrix forms we will define this model as:\n",
    "\n",
    "$$\\hat{Y} = f(X)$$\n",
    "\n",
    "where\n",
    "$$X = \\left(\n",
    "\\begin{array}{cccc}\n",
    "y_{k} & y_{k-1} & \\ldots & y_{1}\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "y_{k+j} & y_{k+j-1} & \\ldots & y_{j+1}\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\n",
    "\\end{array}\n",
    "\\right)$$\n",
    "\n",
    "$$Y = \\left(\n",
    "\\begin{array}{c}\n",
    "y_{k+m} \\\\\n",
    "\\vdots\\\\\n",
    "y_{k+j+m}\\\\\n",
    "\\vdots\n",
    "\\end{array}\n",
    "\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[[data_name]].values\n",
    "\n",
    "ss = StandardScaler()\n",
    "X = ss.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X)\n",
    "plt.title(f'Preprocessed \"{data_name}\" data.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AR_matrices(Y, k, m):\n",
    "    X_AR = []\n",
    "    Y_AR = []\n",
    "    for i in range(len(Y)):\n",
    "        \n",
    "        if i < k-1: continue\n",
    "        if i+m >= len(Y): break\n",
    "        \n",
    "        ax_ar = Y[i+1-k:i+1].reshape(-1, )\n",
    "        X_AR.append(ax_ar)\n",
    "\n",
    "        ay_ar = Y[i+m]#[0]\n",
    "        Y_AR.append(ay_ar)\n",
    "\n",
    "    return np.array(X_AR), np.array(Y_AR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare X and Y matrices\n",
    "k = 1\n",
    "X_AR, Y_AR = AR_matrices(X, k=k, m=1)\n",
    "print('X shape:', X.shape)\n",
    "print('X_AR shape: ', X_AR.shape)\n",
    "print('Y_AR shape: ', Y_AR.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train / test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(X)//2\n",
    "\n",
    "X_AR_train, X_AR_test = X_AR[:N], X_AR[N:]\n",
    "Y_AR_train, Y_AR_test = Y_AR[:N], Y_AR[N:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression().fit(X_AR_train, Y_AR_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_test = model.predict(X_AR_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(Y_AR_test, label='True', alpha=1.)\n",
    "plt.plot(Y_pred_test, label='Prediction', alpha=1.)\n",
    "plt.xticks(size=14)\n",
    "plt.yticks(size=14)\n",
    "plt.legend(loc='best', fontsize=14)\n",
    "plt.grid(b=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error as mse\n",
    "print('MSE loss:', mse(Y_AR_test, Y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "\n",
    "Do the same experiment for large k value -- take $k=30$:\n",
    "\n",
    "* make new datasets;\n",
    "* train new model;\n",
    "* compare graphs and MSE losses.\n",
    "\n",
    "Make sure AR model performs better at higher k values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_k = 30\n",
    "\n",
    "# prepare X and Y matrices\n",
    "X_AR_new_k, Y_AR_new_k = AR_matrices(X, k=new_k, m=1)\n",
    "\n",
    "assert X_AR_new_k.shape == (2073, 30)\n",
    "assert Y_AR_new_k.shape == (2073, 1)\n",
    "\n",
    "print('X shape:', X.shape)\n",
    "print('X_AR shape: ', X_AR_new_k.shape)\n",
    "print('Y_AR shape: ', Y_AR_new_k.shape)\n",
    "\n",
    "X_AR_train_new_k, X_AR_test_new_k = X_AR_new_k[:N], X_AR_new_k[N:]\n",
    "Y_AR_train_new_k, Y_AR_test_new_k = Y_AR_new_k[:N], Y_AR_new_k[N:]\n",
    "\n",
    "assert X_AR_train_new_k.shape == (1051, 30)\n",
    "assert X_AR_test_new_k.shape == (1022, 30)\n",
    "assert Y_AR_train_new_k.shape == (1051, 1)\n",
    "assert Y_AR_test_new_k.shape == (1022, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression().fit(X_AR_train_new_k, Y_AR_train_new_k)\n",
    "\n",
    "Y_pred_test_new_k = model.predict(X_AR_test_new_k)\n",
    "\n",
    "assert Y_pred_test_new_k.shape == (1022, 1)\n",
    "\n",
    "for i in range(Y_AR_test.shape[1]):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.plot(Y_AR_test_new_k[:, i], label='True', alpha=1.)\n",
    "    plt.plot(Y_pred_test_new_k[:, i], label=f'Prediction, k={new_k}', alpha=.7, color='C4')\n",
    "    plt.plot(Y_pred_test[-Y_AR_test_new_k.shape[0]:], label='Prediction, k=1', alpha=1.)\n",
    "    plt.xticks(size=14)\n",
    "    plt.yticks(size=14)\n",
    "    plt.legend(loc='best', fontsize=14)\n",
    "    plt.grid(b=1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('MSE loss k=1:', mse(Y_AR_test[-Y_AR_test_new_k.shape[0]:], Y_pred_test[-Y_AR_test_new_k.shape[0]:]))\n",
    "print(f'MSE loss k={new_k}:', mse(Y_AR_test_new_k, Y_pred_test_new_k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM model for time series forecast\n",
    "\n",
    "Now we use a recurrent neural network to solve the problem (note that for such a simple dataset as it is now, it will most likely be more efficient to use a simple model - linear or polynomial)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeseriesDataset(Dataset):   \n",
    "    '''\n",
    "    Custom Dataset subclass. \n",
    "    Serves as input to DataLoader to transform X \n",
    "      into sequence data using rolling window. \n",
    "    DataLoader using this dataset will output batches \n",
    "      of `(batch_size, seq_len, n_features)` shape.\n",
    "    Suitable as an input to RNNs. \n",
    "    '''\n",
    "    def __init__(self, X: np.ndarray, y: np.ndarray, seq_len: int = 1):\n",
    "        self.X = torch.tensor(X).float()\n",
    "        self.y = torch.tensor(y).float()\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.__len__() - (self.seq_len-1)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (self.X[index:index+self.seq_len], self.y[index+self.seq_len-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataModule(pl.LightningDataModule):\n",
    "    '''\n",
    "    LightningDataModule:\n",
    "    https://pytorch-lightning.readthedocs.io/en/latest/extensions/datamodules.html\n",
    "    '''\n",
    "    def __init__(self, seq_len = 1, batch_size = 128, num_workers=0):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.X_train = None\n",
    "        self.y_train = None\n",
    "        self.X_test = None\n",
    "        self.X_test = None\n",
    "        self.columns = None\n",
    "        self.preprocessing = None\n",
    "\n",
    "    def prepare_data(self):\n",
    "        pass\n",
    "\n",
    "    def setup(\n",
    "        self,\n",
    "        stage=None, \n",
    "        X_train=X_AR_train,\n",
    "        y_train=Y_AR_train,\n",
    "        X_test=X_AR_test,\n",
    "        y_test=Y_AR_test,\n",
    "    ):\n",
    "        # Assign train/test datasets for use in dataloaders\n",
    "        if stage == 'fit' or stage is None:\n",
    "            self.X_train = X_train\n",
    "            self.y_train = y_train.reshape((-1, 1))\n",
    "            self.X_val = X_test\n",
    "            self.y_val = y_test.reshape((-1, 1))\n",
    "        if stage == 'test' or stage is None:\n",
    "            self.X_test = X_test\n",
    "            self.y_test = y_test.reshape((-1, 1))\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        train_dataset = TimeseriesDataset(self.X_train, \n",
    "                                          self.y_train, \n",
    "                                          seq_len=self.seq_len)\n",
    "        train_loader = DataLoader(train_dataset, \n",
    "                                  batch_size = self.batch_size, \n",
    "                                  shuffle = False, \n",
    "                                  num_workers = self.num_workers)\n",
    "        \n",
    "        return train_loader\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        val_dataset = TimeseriesDataset(self.X_val, \n",
    "                                        self.y_val, \n",
    "                                        seq_len=self.seq_len)\n",
    "        val_loader = DataLoader(val_dataset, \n",
    "                                batch_size = self.batch_size, \n",
    "                                shuffle = False, \n",
    "                                num_workers = self.num_workers)\n",
    "\n",
    "        return val_loader\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        test_dataset = TimeseriesDataset(self.X_test, \n",
    "                                         self.y_test, \n",
    "                                         seq_len=self.seq_len)\n",
    "        test_loader = DataLoader(test_dataset, \n",
    "                                 batch_size = self.batch_size, \n",
    "                                 shuffle = False, \n",
    "                                 num_workers = self.num_workers)\n",
    "\n",
    "        return test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM model\n",
    "\n",
    "Our model will consist of several LSTM-layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMRegressor(pl.LightningModule):\n",
    "    '''\n",
    "    Standard PyTorch Lightning module:\n",
    "    https://pytorch-lightning.readthedocs.io/en/latest/lightning_module.html\n",
    "    '''\n",
    "    def __init__(self, \n",
    "                 n_features, \n",
    "                 hidden_size, \n",
    "                 seq_len, \n",
    "                 batch_size,\n",
    "                 num_layers, \n",
    "                 dropout, \n",
    "                 learning_rate,\n",
    "                 criterion):\n",
    "        super(LSTMRegressor, self).__init__()\n",
    "        self.n_features = n_features\n",
    "        self.hidden_size = hidden_size\n",
    "        self.seq_len = seq_len\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.criterion = criterion\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=n_features, \n",
    "                            hidden_size=hidden_size,\n",
    "                            num_layers=num_layers, # number of LSTM-layers.\n",
    "                            dropout=dropout, \n",
    "                            batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # lstm_out = (batch_size, seq_len, hidden_size)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        y_pred = self.linear(lstm_out[:,-1])\n",
    "        return y_pred\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    # since the dataset is rather small, we use test data for validation\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        self.log('val_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        self.log('test_loss', loss)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We aggregate all parameters in one place -- to the dict.\n",
    "This is useful for reporting experiment params to experiment tracking software.\n",
    "\n",
    "In fact, for understanding and reproducibility, researchers try to store all hyperparameters \n",
    "in special YAML configurators.\n",
    "\n",
    "You could read more here: [Data Science in Production — Advanced Python Best Practices](https://medium.com/bcggamma/data-science-python-best-practices-fdb16fdedf82)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = dict(\n",
    "    seq_len = 1,\n",
    "    batch_size = 70, \n",
    "    criterion = nn.MSELoss(),\n",
    "    max_epochs = 20,\n",
    "    n_features = 1,\n",
    "    hidden_size = 100,\n",
    "    num_layers = 1,\n",
    "    dropout = 0.2,\n",
    "    learning_rate = 0.001,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMRegressor(\n",
    "    n_features = p['n_features'],\n",
    "    hidden_size = p['hidden_size'],\n",
    "    seq_len = p['seq_len'],\n",
    "    batch_size = p['batch_size'],\n",
    "    criterion = p['criterion'],\n",
    "    num_layers = p['num_layers'],\n",
    "    dropout = p['dropout'],\n",
    "    learning_rate = p['learning_rate']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "seed_everything(1)\n",
    "\n",
    "csv_logger = CSVLogger('./', name='lstm', version='0'),\n",
    "\n",
    "trainer = Trainer(\n",
    "    max_epochs=p['max_epochs'],\n",
    "    logger=csv_logger,\n",
    "    gpus=1,\n",
    "    log_every_n_steps=1,\n",
    "    progress_bar_refresh_rate=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = DataModule(\n",
    "    seq_len = p['seq_len'],\n",
    "    batch_size = p['batch_size']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.fit(model, dm)\n",
    "trainer.test(model, datamodule=dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = pd.read_csv('./lstm/0/metrics.csv')\n",
    "train_loss = metrics[['train_loss', 'step', 'epoch']][~np.isnan(metrics['train_loss'])]\n",
    "val_loss = metrics[['val_loss', 'epoch']][~np.isnan(metrics['val_loss'])]\n",
    "test_loss = metrics['test_loss'].iloc[-1]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5), dpi=100)\n",
    "axes[0].set_title('Train loss per batch')\n",
    "axes[0].plot(train_loss['step'], train_loss['train_loss'])\n",
    "axes[1].set_title('Validation loss per epoch')\n",
    "axes[1].plot(val_loss['epoch'], val_loss['val_loss'], color='orange')\n",
    "plt.show(block = True)\n",
    "\n",
    "print('MSE:')\n",
    "print(f\"Train loss: {train_loss['train_loss'].iloc[-1]:.3f}\")\n",
    "print(f\"Val loss:   {val_loss['val_loss'].iloc[-1]:.3f}\")\n",
    "print(f'Test loss:  {test_loss:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable \n",
    "\n",
    "X_test_lstm = Variable(torch.tensor(X_AR_test, dtype=torch.float).view(-1, 1, 1))\n",
    "X_test_lstm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Y_pred_test_lstm = model(X_test_lstm).detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(Y_AR_test, label='True', alpha=1.)\n",
    "plt.plot(Y_pred_test_lstm, label='Prediction', alpha=1.)\n",
    "plt.xticks(size=14)\n",
    "plt.yticks(size=14)\n",
    "plt.legend(loc='best', fontsize=14)\n",
    "plt.grid(b=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('MSE loss:', mse(Y_AR_test, Y_pred_test_lstm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3\n",
    "\n",
    "Do the same experiment with LSTM for large k value -- take $k=20$ (in the case of LSTM, this is the length of the sequence):\n",
    "\n",
    "* train new model;\n",
    "* compare graphs and MSE losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 20\n",
    "\n",
    "p = dict(\n",
    "    seq_len = # your code where\n",
    "    batch_size = 50, \n",
    "    criterion = nn.MSELoss(),\n",
    "    max_epochs = 20,\n",
    "    n_features = 1,\n",
    "    hidden_size = 20,\n",
    "    num_layers = 1,\n",
    "    dropout = 0.2,\n",
    "    learning_rate = 0.001,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = DataModule(\n",
    "    seq_len = p['seq_len'],\n",
    "    batch_size = p['batch_size']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMRegressor(\n",
    "    n_features = p['n_features'],\n",
    "    hidden_size = p['hidden_size'],\n",
    "    seq_len = p['seq_len'],\n",
    "    batch_size = p['batch_size'],\n",
    "    criterion = p['criterion'],\n",
    "    num_layers = p['num_layers'],\n",
    "    dropout = p['dropout'],\n",
    "    learning_rate = p['learning_rate']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "seed_everything(1)\n",
    "\n",
    "csv_logger = CSVLogger('./', name='lstm', version='0'),\n",
    "\n",
    "trainer = Trainer(\n",
    "    max_epochs=p['max_epochs'],\n",
    "    logger=csv_logger,\n",
    "    gpus=1,\n",
    "    log_every_n_steps=1,\n",
    "    progress_bar_refresh_rate=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trainer.fit(model, dm)\n",
    "trainer.test(model, datamodule=dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = pd.read_csv('./lstm/0/metrics.csv')\n",
    "train_loss = metrics[['train_loss', 'step', 'epoch']][~np.isnan(metrics['train_loss'])]\n",
    "val_loss = metrics[['val_loss', 'epoch']][~np.isnan(metrics['val_loss'])]\n",
    "test_loss = metrics['test_loss'].iloc[-1]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5), dpi=100)\n",
    "axes[0].set_title('Train loss per batch')\n",
    "axes[0].plot(train_loss['step'], train_loss['train_loss'])\n",
    "axes[1].set_title('Validation loss per epoch')\n",
    "axes[1].plot(val_loss['epoch'], val_loss['val_loss'], color='orange')\n",
    "plt.show(block = True)\n",
    "\n",
    "print('MSE:')\n",
    "print(f\"Train loss: {train_loss['train_loss'].iloc[-1]:.3f}\")\n",
    "print(f\"Val loss:   {val_loss['val_loss'].iloc[-1]:.3f}\")\n",
    "print(f'Test loss:  {test_loss:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_AR_test_new_k_lstm = # your code here\n",
    "assert X_AR_test_new_k_lstm.shape == torch.Size([1022, 30, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Y_pred_test_new_k_lstm = # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(Y_AR_test.shape[1]):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.plot(Y_AR_test, label='True', alpha=1.)\n",
    "    plt.plot(Y_pred_test_new_k_lstm, label=f'Prediction, k={new_k}', alpha=.7, color='C4')\n",
    "    plt.plot(Y_pred_test_lstm, label='Prediction, k=1', alpha=1.)\n",
    "    plt.xticks(size=14)\n",
    "    plt.yticks(size=14)\n",
    "    plt.legend(loc='best', fontsize=14)\n",
    "    plt.grid(b=1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('MSE loss k=1:', mse(Y_pred_test_lstm, Y_AR_test))\n",
    "print(f'MSE loss k={seq_len}:', mse(Y_pred_test_new_k_lstm, Y_AR_test_new_k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In this notebook, we have learned to use predict a rather toy-like time series using linear models and recurrent neural networks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
