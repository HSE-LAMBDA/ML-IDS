{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quality metrics for classification\n",
    "\n",
    "We will consider a range of classification quality metrics solving a classification problem with a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.testing as np_testing\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load MAGIC Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"img/magic1.jpg\" width=\"1000\"></center>\n",
    "\n",
    "Source: https://magic.mpp.mpg.de/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/magic/magic04.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features description:\n",
    "- **Length:** continuous # major axis of ellipse [mm]\n",
    "- **Width:** continuous # minor axis of ellipse [mm]\n",
    "- **Size:** continuous # 10-log of sum of content of all pixels [in #phot]\n",
    "- **Conc:** continuous # ratio of sum of two highest pixels over fSize [ratio]\n",
    "- **Conc1:** continuous # ratio of highest pixel over fSize [ratio]\n",
    "- **Asym:** continuous # distance from highest pixel to center, projected onto major axis [mm]\n",
    "- **M3Long:** continuous # 3rd root of third moment along major axis [mm]\n",
    "- **M3Trans:** continuous # 3rd root of third moment along minor axis [mm]\n",
    "- **Alpha:** continuous # angle of major axis with vector to origin [deg]\n",
    "- **Dist:** continuous # distance from origin to center of ellipse [mm]\n",
    "- **Label:** g,h # gamma (signal), hadron (background)\n",
    "\n",
    "g = gamma (signal): 12332 \\\n",
    "h = hadron (background): 6688"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "f_names = np.array([\"Length\", \"Width\", \"Size\", \"Conc\", \"Conc1\", \"Asym\", \"M3Long\", \"M3Trans\", \"Alpha\", \"Dist\"])\n",
    "\n",
    "data = pd.read_csv(\"magic04.data\", header=None, names=list(f_names)+[\"Label\"])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare a matrix of input features\n",
    "X = data[f_names].values\n",
    "\n",
    "# prepare a vector of true labels\n",
    "y = 1 * (data['Label'].values == \"g\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print sizes of X and y\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train / test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into train and test samples\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale input data using StandardScaler:\n",
    "$$\n",
    "X_{new} = \\frac{X - \\mu}{\\sigma}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Import StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create object of the class and set up its parameters\n",
    "ss = StandardScaler()\n",
    "\n",
    "# Estimate mean and sigma values\n",
    "ss.fit(X_train)\n",
    "\n",
    "# Scale train and test samples\n",
    "X_train = ss.transform(X_train)\n",
    "X_test = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit a classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a neural network and fit it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pytorch-lightning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch import nn\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "class Model(pl.LightningModule):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # define all layers of the netwrok\n",
    "        self.net = nn.Sequential(\n",
    "                                nn.Linear(10, 10), \n",
    "                                nn.Tanh(), \n",
    "                                nn.Linear(10, 1), \n",
    "                                nn.Sigmoid())\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # make a prediction for x\n",
    "        return self.net(x)\n",
    "\n",
    "    # calculate loss function values\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.binary_cross_entropy(y_hat, y)\n",
    "        return loss\n",
    "\n",
    "    # define optimizer to fit the network\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# combine X and y into one pytorch tensor dataset\n",
    "dataset_train = TensorDataset(torch.tensor(X_train, dtype=torch.float), \n",
    "                              torch.tensor(y_train.reshape(-1, 1), dtype=torch.float))\n",
    "\n",
    "# loader divides our train data into batches\n",
    "train_loader = DataLoader(dataset_train, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define trainer to fit out network\n",
    "trainer = pl.Trainer(max_epochs=10)\n",
    "\n",
    "# init our netwrok\n",
    "model = Model()\n",
    "\n",
    "# fit the netwrok\n",
    "trainer.fit(model, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make prediction of **probability** of the positive class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# make predictions\n",
    "y_proba_test = model(torch.tensor(X_test, dtype=torch.float))[:, 0].detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"Truth  : \", y_test[:10])\n",
    "print(\"Proba  : \", y_proba_test[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make prediction of class **label**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# transform the predicted probabilities into predicted labels {0, 1}\n",
    "y_pred_test = 1 * (y_proba_test > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"Truth  : \", y_test[:10])\n",
    "print(\"Pred   : \", y_pred_test[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label-based Quality Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a confusion matrix:\n",
    "\n",
    "<center><img src='img/cm.png'></center>\n",
    "\n",
    "\n",
    "* TP (true positive) - currectly predicted positives\n",
    "* FP (false positive) - incorrectly predicted negatives (1st order error)\n",
    "* FN (false negative) - incorrectly predicted positives (2nd order error)\n",
    "* TN (true negative) - currectly predicted negatives\n",
    "* Pos (Neg) - total number of positives (negatives)\n",
    "\n",
    "Quality metrics:\n",
    "\n",
    "* $ \\text{Accuracy} = \\frac{TP + TN}{Pos+Neg}$\n",
    "* $ \\text{Error rate} = 1 -\\text{accuracy}$\n",
    "* $ \\text{Precision} =\\frac{TP}{TP + FP}$ \n",
    "* $ \\text{Recall} =\\frac{TP}{TP + FN} = \\frac{TP}{Pos}$\n",
    "* $ \\text{F}_\\beta \\text{-score} = (1 + \\beta^2) \\cdot \\frac{\\mathrm{precision} \\cdot \\mathrm{recall}}{(\\beta^2 \\cdot \\mathrm{precision}) + \\mathrm{recall}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1\n",
    "Complete a function that computes TP, FP, TN, FN, Accuracy, Error rate, Precision, Recall and F1-score metrics for a classifier.\n",
    "\n",
    "**Hint:** use implementation of the metrics from `sklearn.metrics` as it is shown below. Example for confusin matrix: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html#sklearn.metrics.confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "9454d8_answer",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def quality_metrics_report(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true: array-like of shape (n_samples,)\n",
    "        Ground truth (correct) target values.\n",
    "    y_pred: array-like of shape (n_samples,)\n",
    "        Estimated targets as returned by a classifier.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    List of metric values: [tp, fp, fn, tn, accuracy, error_rate, precision, recall, f1]\n",
    "    \"\"\"\n",
    "    \n",
    "    ### BEGIN SOLUTION\n",
    "    \n",
    "    ### END SOLUTION\n",
    "    \n",
    "    return [tp, fp, fn, tn, accuracy, error_rate, precision, recall, f1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "quality_metrics_report([0, 1, 0, 1], [1, 1, 1, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "<center>   \n",
    "    \n",
    "```python\n",
    "[1, 2, 1, 0, 0.25, 0.75, 0.3333333333333333, 0.5, 0.4]\n",
    "    \n",
    "``` \n",
    "    \n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "nbgrader": {
     "grade": true,
     "grade_id": "9454d8",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN HIDDEN TESTS\n",
    "actual  = quality_metrics_report([0, 1, 0, 1], [1, 1, 1, 0])\n",
    "desired = [1, 2, 1, 0, 0.25, 0.75, 0.3333333333333333, 0.5, 0.4]\n",
    "np_testing.assert_almost_equal(actual, desired, decimal=1)\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compute all these quality metrics for all classifiers considered above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "metrics_report = pd.DataFrame(columns=['TP', 'FP', 'FN', 'TN', 'Accuracy', 'Error rate', 'Precision', 'Recall', 'F1'])\n",
    "\n",
    "metrics_report.loc['Model', :] = quality_metrics_report(y_test, y_pred_test)\n",
    "\n",
    "metrics_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability-based Quality Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC curve\n",
    "\n",
    "The receiver operating characteristic curve (ROC) measures how well a classifier separates two classes. \n",
    "\n",
    "Let $y_{\\rm i}$ is a true label and $\\hat{y}_{\\rm i}$ is a predicted score for the $i^{\\rm th}$ observation. \n",
    "\n",
    "The numbers of positive and negative observations: $\\mathcal{I}_{\\rm 1} = \\{i: y_{\\rm i}=1\\}$ and $\\mathcal{I}_{\\rm 0} = \\{i: y_{\\rm i}=0\\}$. \n",
    "\n",
    "The sum of observation weights for each class: $W_{\\rm 1} = \\sum_{i \\in \\mathcal{I}_{\\rm 1}} w_{\\rm i}$ and  $W_{\\rm 0} = \\sum_{i \\in \\mathcal{I}_{\\rm 0}} w_{\\rm i}$. \n",
    "\n",
    "For each predicted score threshold value $\\tau$, True Positive Rate (TPR) and False Positive Rate (FPR) are calculated:\n",
    "\n",
    "\\begin{equation}\n",
    "TPR(\\tau) = \\frac{1}{W_{\\rm 1}} \\sum_{i \\in \\mathcal{I}_{\\rm 1}} I[\\hat{y}_{\\rm i} \\ge \\tau] w_{\\rm i}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "FPR(\\tau) = \\frac{1}{W_{\\rm 0}} \\sum_{i \\in \\mathcal{I}_{\\rm 0}} I[\\hat{y}_{\\rm i} \\ge \\tau] w_{\\rm i}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2\n",
    "Complete the fucntion below, that computes a ROC curve and ROC AUC for a classifier.\n",
    "\n",
    "**Hint:** use `roc_curve` and `auc` from `from sklearn.metrics`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "7dc45a_answer",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "def roc_curve_report(y_true, y_proba):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true: array-like of shape (n_samples,)\n",
    "        Ground truth (correct) target values.\n",
    "    y_proba: array-like of shape (n_samples,)\n",
    "        Predicted probabilities of the positive class predicted by a classifier.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    fpr : array, shape = [>2]\n",
    "        Increasing false positive rates such that element i is the false\n",
    "        positive rate of predictions with score >= thresholds[i].\n",
    "    tpr : array, shape = [>2]\n",
    "        Increasing true positive rates such that element i is the true\n",
    "        positive rate of predictions with score >= thresholds[i].\n",
    "    roc_auc : float\n",
    "        Area under the ROC curve defined by the fpr and tpr.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### BEGIN SOLUTION\n",
    "\n",
    "    ### END SOLUTION\n",
    "    \n",
    "    return fpr, tpr, roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "roc_curve_report([0, 1, 0, 1], [0.6, 0.9, 0.1, 0.4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "<center>   \n",
    "    \n",
    "```python\n",
    "(array([0. , 0. , 0.5, 0.5, 1. ]), array([0. , 0.5, 0.5, 1. , 1. ]), 0.75)\n",
    "    \n",
    "``` \n",
    "    \n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "nbgrader": {
     "grade": true,
     "grade_id": "7dc45a",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN HIDDEN TESTS\n",
    "actual  = roc_curve_report([0, 1, 0, 1], [0.6, 0.9, 0.1, 0.4])[0]\n",
    "desired = np.array([0. , 0. , 0.5, 0.5, 1. ])\n",
    "np_testing.assert_almost_equal(actual, desired, decimal=1)\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot ROC curves for all classifiers considered above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "fpr, tpr, roc_auc = roc_curve_report(y_test, y_proba_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 6))\n",
    "plt.plot(fpr, tpr, linewidth=3, label='Model')\n",
    "\n",
    "plt.xlabel('FPR', size=18)\n",
    "plt.ylabel('TPR', size=18)\n",
    "\n",
    "plt.legend(loc='best', fontsize=14)\n",
    "plt.grid(b=1)\n",
    "plt.show()\n",
    "\n",
    "print('ROC AUC:', roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
